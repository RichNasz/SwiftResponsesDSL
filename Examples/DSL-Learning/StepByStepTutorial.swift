//
//  StepByStepTutorial.swift
//  SwiftResponsesDSL Examples
//
//  Comprehensive step-by-step tutorial for learning SwiftResponsesDSL
//  from absolute basics to advanced usage patterns.
//
//  Generated by AI-assisted code generation.
//  Created by Richard Naszcyniec on [Date].
//  Copyright Â© [Year] Richard Naszcyniec. All rights reserved.
//

import SwiftResponsesDSL

/// Tutorial Step 1: Your First Request
/// The absolute minimum to get started with SwiftResponsesDSL
func step1FirstRequest() async throws {
    print("ğŸš€ Step 1: Your First Request")
    print("============================")

    print("Goal: Send your first message to an LLM and get a response")
    print("Concepts: LLMClient, ResponseRequest, basic response handling")
    print()

    // Step 1.1: Create a client
    print("1ï¸âƒ£ Create an LLM Client:")
    print("   This connects to your LLM API (OpenAI, etc.)")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")
    print("   âœ… Client created successfully")
    print()

    // Step 1.2: Create a simple request
    print("2ï¸âƒ£ Create a Request:")
    print("   The simplest possible request with just a user message")

    let request = try ResponseRequest(
        model: "gpt-4",           // Which LLM to use
        input: {
            user("Hello!")        // What to ask
        }
    )
    print("   âœ… Request created with user message")
    print()

    // Step 1.3: Send and receive
    print("3ï¸âƒ£ Send Request & Get Response:")
    print("   Execute the request and handle the response")

    let response = try await client.respond(to: request)
    print("   ğŸ“¡ Request sent to API...")
    print()

    // Step 1.4: Extract the answer
    print("4ï¸âƒ£ Extract the Answer:")
    print("   Get the actual response text from the API")

    if let message = response.choices.first?.message.content {
        print("   ğŸ¤– Assistant says: '\(message)'")
    } else {
        print("   ğŸ¤– (No response received)")
    }
    print()

    // Step 1.5: Summary
    print("ğŸ¯ What You Learned:")
    print("   â€¢ How to create an LLMClient")
    print("   â€¢ How to construct a ResponseRequest")
    print("   â€¢ How to send requests and receive responses")
    print("   â€¢ Basic error handling (try/catch)")
    print()

    print("âœ… Step 1 completed! You can now send basic messages to LLMs.")
    print("ğŸ’¡ Next: Learn about configuration parameters.")
}

/// Tutorial Step 2: Adding Configuration
/// Learn how to control LLM behavior with parameters
func step2AddingConfiguration() async throws {
    print("\nâš™ï¸  Step 2: Adding Configuration")
    print("==============================")

    print("Goal: Control how the LLM responds using configuration parameters")
    print("Concepts: Temperature, MaxOutputTokens, response control")
    print()

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Step 2.1: Temperature control
    print("1ï¸âƒ£ Temperature Control:")
    print("   Controls creativity/randomness (0.0 = focused, 2.0 = creative)")

    let temperatures = [0.1, 1.5]
    let question = "Describe a cat in one sentence"

    for temp in temperatures {
        print("   ğŸŒ¡ï¸ Temperature \(temp):")

        let request = try ResponseRequest(
            model: "gpt-4",
            config: {
                Temperature(temp)
            },
            input: {
                user(question)
            }
        )

        let response = try await client.respond(to: request)
        if let answer = response.choices.first?.message.content {
            print("      ğŸ¤– \(answer)")
        }
        print()
    }

    // Step 2.2: Response length control
    print("2ï¸âƒ£ Response Length Control:")
    print("   Limit how much the LLM can respond")

    let lengths = [20, 100]
    let topic = "Explain photosynthesis"

    for maxTokens in lengths {
        print("   ğŸ“ Max \(maxTokens) tokens:")

        let request = try ResponseRequest(
            model: "gpt-4",
            config: {
                MaxOutputTokens(maxTokens)
            },
            input: {
                user(topic)
            }
        )

        let response = try await client.respond(to: request)
        if let answer = response.choices.first?.message.content {
            let wordCount = answer.split(separator: " ").count
            print("      ğŸ¤– (\(wordCount) words) \(answer.prefix(80))...")
        }
        print()
    }

    // Step 2.3: Combining parameters
    print("3ï¸âƒ£ Combining Multiple Parameters:")
    print("   Create the perfect configuration for your use case")

    let optimalRequest = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.7)        // Balanced creativity
            MaxOutputTokens(150)    // Reasonable length
            TopP(0.9)              // Nucleus sampling
            FrequencyPenalty(0.1)   // Reduce repetition
        },
        input: {
            user("Write a creative story about a robot learning to paint")
        }
    )

    print("   âš™ï¸ Combined config: Temperature 0.7, Max 150 tokens, TopP 0.9, FrequencyPenalty 0.1")

    let response = try await client.respond(to: optimalRequest)
    if let story = response.choices.first?.message.content {
        print("   ğŸ¤– Story: \(story.prefix(100))...")
    }
    print()

    // Step 2.4: Parameter validation
    print("4ï¸âƒ£ Parameter Validation:")
    print("   DSL prevents invalid configurations at compile time")

    do {
        let invalidRequest = try ResponseRequest(
            model: "gpt-4",
            config: {
                Temperature(5.0)  // Invalid: > 2.0
            },
            input: {
                user("Hello")
            }
        )
        print("   âŒ This shouldn't succeed")

    } catch LLMError.invalidValue(let message) {
        print("   âœ… Validation caught error: \(message)")
    }
    print()

    print("ğŸ¯ What You Learned:")
    print("   â€¢ Temperature controls creativity")
    print("   â€¢ MaxOutputTokens limits response length")
    print("   â€¢ How to combine multiple parameters")
    print("   â€¢ Parameter validation prevents errors")
    print()

    print("âœ… Step 2 completed! You can now control LLM behavior.")
    print("ğŸ’¡ Next: Learn about conversation management.")
}

/// Tutorial Step 3: Managing Conversations
/// Learn how to maintain context across multiple interactions
func step3ConversationManagement() async throws {
    print("\nğŸ’¬ Step 3: Conversation Management")
    print("=================================")

    print("Goal: Maintain context across multiple messages")
    print("Concepts: ResponseConversation, message history, context preservation")
    print()

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Step 3.1: Creating a conversation
    print("1ï¸âƒ£ Create a Conversation:")
    print("   Conversations remember the full context of your chat")

    var conversation = ResponseConversation()
    print("   âœ… Empty conversation created")
    print()

    // Step 3.2: Adding context
    print("2ï¸âƒ£ Add Context:")
    print("   Set up the assistant's role and personality")

    conversation.append(system: "You are a patient programming tutor who explains concepts step by step")
    print("   âœ… System context added")
    print()

    // Step 3.3: First exchange
    print("3ï¸âƒ£ First Exchange:")
    print("   Ask a question and get a response")

    conversation.append(user: "What is recursion?")

    let firstResponse = try await client.chat(conversation: conversation)
    conversation.append(response: firstResponse)

    if let answer = firstResponse.choices.first?.message.content {
        print("   ğŸ‘¤ User: What is recursion?")
        print("   ğŸ¤– Assistant: \(answer.prefix(100))...")
    }
    print()

    // Step 3.4: Follow-up questions
    print("4ï¸âƒ£ Follow-up Questions:")
    print("   Ask related questions - assistant remembers context")

    let followUps = [
        "Can you show me a simple example?",
        "How does it compare to a loop?"
    ]

    for followUp in followUps {
        print("   ğŸ‘¤ User: \(followUp)")

        conversation.append(user: followUp)

        let response = try await client.chat(conversation: conversation)
        conversation.append(response: response)

        if let answer = response.choices.first?.message.content {
            print("   ğŸ¤– Assistant: \(answer.prefix(100))...")
        }
        print()
    }

    // Step 3.5: Conversation summary
    print("5ï¸âƒ£ Conversation Summary:")
    print("   ğŸ“Š Total messages: \(conversation.messages.count)")
    print("   ğŸ¤– Assistant responses: \(conversation.messages.filter { $0.role == .assistant }.count)")
    print("   ğŸ‘¤ User questions: \(conversation.messages.filter { $0.role == .user }.count)")
    print("   âš™ï¸ System context: \(conversation.messages.filter { $0.role == .system }.count)")
    print()

    // Step 3.6: Manual conversation construction
    print("6ï¸âƒ£ Manual Construction:")
    print("   You can also build conversations manually")

    let manualConversation = ResponseConversation()
    manualConversation.append(system: "You are a creative writer")
    manualConversation.append(user: "Write a haiku about coding")

    let manualResponse = try await client.chat(conversation: manualConversation)

    if let haiku = manualResponse.choices.first?.message.content {
        print("   ğŸ“ Manual conversation result:")
        print("   ğŸ¤– \(haiku)")
    }
    print()

    print("ğŸ¯ What You Learned:")
    print("   â€¢ How to create and manage conversations")
    print("   â€¢ Adding system context for personality")
    print("   â€¢ Maintaining context across exchanges")
    print("   â€¢ Manual vs DSL conversation construction")
    print()

    print("âœ… Step 3 completed! You can now manage multi-turn conversations.")
    print("ğŸ’¡ Next: Learn about advanced features.")
}

/// Tutorial Step 4: Advanced Features
/// Learn streaming, error handling, and complex configurations
func step4AdvancedFeatures() async throws {
    print("\nğŸš€ Step 4: Advanced Features")
    print("===========================")

    print("Goal: Master advanced SwiftResponsesDSL features")
    print("Concepts: Streaming, error handling, complex configurations")
    print()

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Step 4.1: Streaming responses
    print("1ï¸âƒ£ Streaming Responses:")
    print("   Get responses in real-time as they're generated")

    let streamingRequest = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.7)
            MaxOutputTokens(100)
            StreamOptions(["include_usage": true])
        },
        input: {
            user("Write a short story about a dragon who loves gardening")
        }
    )

    print("   ğŸŒŠ Streaming response:")
    print("   ğŸ¤– ", terminator: "")

    let stream = client.stream(request: streamingRequest)
    var totalContent = ""

    for try await event in stream {
        switch event {
        case .outputItemAdded(let item):
            if case .message(let message) = item,
               let content = message.content {
                totalContent += content
                print(content, terminator: "")
                fflush(stdout)
            }

        case .completed(let response):
            print("\n   âœ… Streaming completed!")
            if let usage = response.usage {
                print("   ğŸ“Š Tokens used: \(usage.totalTokens)")
            }

        case .created:
            print("(Stream started) ", terminator: "")

        default:
            break
        }
    }
    print()

    // Step 4.2: Error handling
    print("2ï¸âƒ£ Error Handling:")
    print("   Handle different types of errors gracefully")

    do {
        // Try with invalid model
        let badRequest = try ResponseRequest(
            model: "invalid-model-name",
            input: {
                user("Hello")
            }
        )

        let response = try await client.respond(to: badRequest)
        print("   ğŸ¤– Response: \(response.choices.first?.message.content ?? "None")")

    } catch LLMError.invalidModel {
        print("   âœ… Caught invalid model error")
    } catch LLMError.networkError(let message) {
        print("   âœ… Caught network error: \(message)")
    } catch {
        print("   âœ… Caught other error: \(error.localizedDescription)")
    }
    print()

    // Step 4.3: Complex configurations
    print("3ï¸âƒ£ Complex Configurations:")
    print("   Combine many parameters for specific use cases")

    let creativeWritingRequest = try ResponseRequest(
        model: "gpt-4",
        config: {
            // Creativity settings
            Temperature(0.9)
            TopP(0.95)
            FrequencyPenalty(0.3)

            // Length control
            MaxOutputTokens(300)

            // Quality settings
            PresencePenalty(0.2)
        },
        input: {
            system("You are a bestselling novelist specializing in science fiction")
            user("Write the opening paragraph of a novel about AI achieving consciousness")
        }
    )

    let creativeResponse = try await client.respond(to: creativeWritingRequest)

    if let opening = creativeResponse.choices.first?.message.content {
        print("   ğŸ“– Creative writing result:")
        print("   ğŸ¤– \(opening.prefix(200))...")
    }
    print()

    // Step 4.4: Multiple response choices
    print("4ï¸âƒ£ Multiple Response Choices:")
    print("   Generate several different responses to the same prompt")

    let multiChoiceRequest = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.8)  // Higher temperature for variety
        },
        input: {
            user("Suggest a creative name for a new programming language")
        }
    )

    // Note: In a real implementation, you'd set n parameter for multiple choices
    let singleResponse = try await client.respond(to: multiChoiceRequest)

    if let suggestion = singleResponse.choices.first?.message.content {
        print("   ğŸ’¡ Language name suggestion:")
        print("   ğŸ¤– \(suggestion)")
        print("   ğŸ“ (In production, you could request multiple choices)")
    }
    print()

    print("ğŸ¯ What You Learned:")
    print("   â€¢ Real-time streaming responses")
    print("   â€¢ Comprehensive error handling")
    print("   â€¢ Complex parameter combinations")
    print("   â€¢ Multiple response strategies")
    print()

    print("âœ… Step 4 completed! You now know advanced SwiftResponsesDSL features.")
    print("ğŸ‰ Congratulations! You have completed the comprehensive tutorial!")
}

/// Main tutorial runner
func runCompleteTutorial() async {
    print("ğŸ“š SwiftResponsesDSL - Complete Step-by-Step Tutorial")
    print("===================================================")
    print("This tutorial will take you from zero to hero with SwiftResponsesDSL.")
    print("Each step builds on the previous one, introducing new concepts.\n")

    do {
        try await step1FirstRequest()
        try await step2AddingConfiguration()
        try await step3ConversationManagement()
        try await step4AdvancedFeatures()

        print("\nğŸŠ TUTORIAL COMPLETE!")
        print("=====================")
        print("ğŸ¯ You now know how to:")
        print("   âœ… Send basic messages to LLMs")
        print("   âœ… Control LLM behavior with parameters")
        print("   âœ… Manage multi-turn conversations")
        print("   âœ… Use advanced features like streaming")
        print("   âœ… Handle errors gracefully")
        print("   âœ… Create complex configurations")
        print()
        print("ğŸš€ Ready to build amazing AI-powered applications!")
        print("ğŸ“– Explore other example folders for more specialized use cases.")

    } catch {
        print("âŒ Tutorial failed with error:", error.localizedDescription)
        print("ğŸ’¡ Make sure you have:")
        print("   â€¢ Valid API credentials")
        print("   â€¢ Internet connection")
        print("   â€¢ Correct endpoint URL")
    }
}
