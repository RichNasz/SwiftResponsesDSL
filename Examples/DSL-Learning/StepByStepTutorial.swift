//
//  StepByStepTutorial.swift
//  SwiftResponsesDSL Examples
//
//  Comprehensive step-by-step tutorial for learning SwiftResponsesDSL
//  from absolute basics to advanced usage patterns.
//
//  Generated by AI-assisted code generation.
//  Created by Richard Naszcyniec on [Date].
//  Copyright © [Year] Richard Naszcyniec. All rights reserved.
//

import SwiftResponsesDSL

/// Tutorial Step 1: Your First Request
/// The absolute minimum to get started with SwiftResponsesDSL
func step1FirstRequest() async throws {
    print("🚀 Step 1: Your First Request")
    print("============================")

    print("Goal: Send your first message to an LLM and get a response")
    print("Concepts: LLMClient, ResponseRequest, basic response handling")
    print()

    // Step 1.1: Create a client
    print("1️⃣ Create an LLM Client:")
    print("   This connects to your LLM API (OpenAI, etc.)")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")
    print("   ✅ Client created successfully")
    print()

    // Step 1.2: Create a simple request
    print("2️⃣ Create a Request:")
    print("   The simplest possible request with just a user message")

    let request = try ResponseRequest(
        model: "gpt-4",           // Which LLM to use
        input: {
            user("Hello!")        // What to ask
        }
    )
    print("   ✅ Request created with user message")
    print()

    // Step 1.3: Send and receive
    print("3️⃣ Send Request & Get Response:")
    print("   Execute the request and handle the response")

    let response = try await client.respond(to: request)
    print("   📡 Request sent to API...")
    print()

    // Step 1.4: Extract the answer
    print("4️⃣ Extract the Answer:")
    print("   Get the actual response text from the API")

    if let message = response.choices.first?.message.content {
        print("   🤖 Assistant says: '\(message)'")
    } else {
        print("   🤖 (No response received)")
    }
    print()

    // Step 1.5: Summary
    print("🎯 What You Learned:")
    print("   • How to create an LLMClient")
    print("   • How to construct a ResponseRequest")
    print("   • How to send requests and receive responses")
    print("   • Basic error handling (try/catch)")
    print()

    print("✅ Step 1 completed! You can now send basic messages to LLMs.")
    print("💡 Next: Learn about configuration parameters.")
}

/// Tutorial Step 2: Adding Configuration
/// Learn how to control LLM behavior with parameters
func step2AddingConfiguration() async throws {
    print("\n⚙️  Step 2: Adding Configuration")
    print("==============================")

    print("Goal: Control how the LLM responds using configuration parameters")
    print("Concepts: Temperature, MaxOutputTokens, response control")
    print()

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Step 2.1: Temperature control
    print("1️⃣ Temperature Control:")
    print("   Controls creativity/randomness (0.0 = focused, 2.0 = creative)")

    let temperatures = [0.1, 1.5]
    let question = "Describe a cat in one sentence"

    for temp in temperatures {
        print("   🌡️ Temperature \(temp):")

        let request = try ResponseRequest(
            model: "gpt-4",
            config: {
                Temperature(temp)
            },
            input: {
                user(question)
            }
        )

        let response = try await client.respond(to: request)
        if let answer = response.choices.first?.message.content {
            print("      🤖 \(answer)")
        }
        print()
    }

    // Step 2.2: Response length control
    print("2️⃣ Response Length Control:")
    print("   Limit how much the LLM can respond")

    let lengths = [20, 100]
    let topic = "Explain photosynthesis"

    for maxTokens in lengths {
        print("   📏 Max \(maxTokens) tokens:")

        let request = try ResponseRequest(
            model: "gpt-4",
            config: {
                MaxOutputTokens(maxTokens)
            },
            input: {
                user(topic)
            }
        )

        let response = try await client.respond(to: request)
        if let answer = response.choices.first?.message.content {
            let wordCount = answer.split(separator: " ").count
            print("      🤖 (\(wordCount) words) \(answer.prefix(80))...")
        }
        print()
    }

    // Step 2.3: Combining parameters
    print("3️⃣ Combining Multiple Parameters:")
    print("   Create the perfect configuration for your use case")

    let optimalRequest = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.7)        // Balanced creativity
            MaxOutputTokens(150)    // Reasonable length
            TopP(0.9)              // Nucleus sampling
            FrequencyPenalty(0.1)   // Reduce repetition
        },
        input: {
            user("Write a creative story about a robot learning to paint")
        }
    )

    print("   ⚙️ Combined config: Temperature 0.7, Max 150 tokens, TopP 0.9, FrequencyPenalty 0.1")

    let response = try await client.respond(to: optimalRequest)
    if let story = response.choices.first?.message.content {
        print("   🤖 Story: \(story.prefix(100))...")
    }
    print()

    // Step 2.4: Parameter validation
    print("4️⃣ Parameter Validation:")
    print("   DSL prevents invalid configurations at compile time")

    do {
        let invalidRequest = try ResponseRequest(
            model: "gpt-4",
            config: {
                Temperature(5.0)  // Invalid: > 2.0
            },
            input: {
                user("Hello")
            }
        )
        print("   ❌ This shouldn't succeed")

    } catch LLMError.invalidValue(let message) {
        print("   ✅ Validation caught error: \(message)")
    }
    print()

    print("🎯 What You Learned:")
    print("   • Temperature controls creativity")
    print("   • MaxOutputTokens limits response length")
    print("   • How to combine multiple parameters")
    print("   • Parameter validation prevents errors")
    print()

    print("✅ Step 2 completed! You can now control LLM behavior.")
    print("💡 Next: Learn about conversation management.")
}

/// Tutorial Step 3: Managing Conversations
/// Learn how to maintain context across multiple interactions
func step3ConversationManagement() async throws {
    print("\n💬 Step 3: Conversation Management")
    print("=================================")

    print("Goal: Maintain context across multiple messages")
    print("Concepts: ResponseConversation, message history, context preservation")
    print()

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Step 3.1: Creating a conversation
    print("1️⃣ Create a Conversation:")
    print("   Conversations remember the full context of your chat")

    var conversation = ResponseConversation()
    print("   ✅ Empty conversation created")
    print()

    // Step 3.2: Adding context
    print("2️⃣ Add Context:")
    print("   Set up the assistant's role and personality")

    conversation.append(system: "You are a patient programming tutor who explains concepts step by step")
    print("   ✅ System context added")
    print()

    // Step 3.3: First exchange
    print("3️⃣ First Exchange:")
    print("   Ask a question and get a response")

    conversation.append(user: "What is recursion?")

    let firstResponse = try await client.chat(conversation: conversation)
    conversation.append(response: firstResponse)

    if let answer = firstResponse.choices.first?.message.content {
        print("   👤 User: What is recursion?")
        print("   🤖 Assistant: \(answer.prefix(100))...")
    }
    print()

    // Step 3.4: Follow-up questions
    print("4️⃣ Follow-up Questions:")
    print("   Ask related questions - assistant remembers context")

    let followUps = [
        "Can you show me a simple example?",
        "How does it compare to a loop?"
    ]

    for followUp in followUps {
        print("   👤 User: \(followUp)")

        conversation.append(user: followUp)

        let response = try await client.chat(conversation: conversation)
        conversation.append(response: response)

        if let answer = response.choices.first?.message.content {
            print("   🤖 Assistant: \(answer.prefix(100))...")
        }
        print()
    }

    // Step 3.5: Conversation summary
    print("5️⃣ Conversation Summary:")
    print("   📊 Total messages: \(conversation.messages.count)")
    print("   🤖 Assistant responses: \(conversation.messages.filter { $0.role == .assistant }.count)")
    print("   👤 User questions: \(conversation.messages.filter { $0.role == .user }.count)")
    print("   ⚙️ System context: \(conversation.messages.filter { $0.role == .system }.count)")
    print()

    // Step 3.6: Manual conversation construction
    print("6️⃣ Manual Construction:")
    print("   You can also build conversations manually")

    let manualConversation = ResponseConversation()
    manualConversation.append(system: "You are a creative writer")
    manualConversation.append(user: "Write a haiku about coding")

    let manualResponse = try await client.chat(conversation: manualConversation)

    if let haiku = manualResponse.choices.first?.message.content {
        print("   📝 Manual conversation result:")
        print("   🤖 \(haiku)")
    }
    print()

    print("🎯 What You Learned:")
    print("   • How to create and manage conversations")
    print("   • Adding system context for personality")
    print("   • Maintaining context across exchanges")
    print("   • Manual vs DSL conversation construction")
    print()

    print("✅ Step 3 completed! You can now manage multi-turn conversations.")
    print("💡 Next: Learn about advanced features.")
}

/// Tutorial Step 4: Advanced Features
/// Learn streaming, error handling, and complex configurations
func step4AdvancedFeatures() async throws {
    print("\n🚀 Step 4: Advanced Features")
    print("===========================")

    print("Goal: Master advanced SwiftResponsesDSL features")
    print("Concepts: Streaming, error handling, complex configurations")
    print()

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Step 4.1: Streaming responses
    print("1️⃣ Streaming Responses:")
    print("   Get responses in real-time as they're generated")

    let streamingRequest = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.7)
            MaxOutputTokens(100)
            StreamOptions(["include_usage": true])
        },
        input: {
            user("Write a short story about a dragon who loves gardening")
        }
    )

    print("   🌊 Streaming response:")
    print("   🤖 ", terminator: "")

    let stream = client.stream(request: streamingRequest)
    var totalContent = ""

    for try await event in stream {
        switch event {
        case .outputItemAdded(let item):
            if case .message(let message) = item,
               let content = message.content {
                totalContent += content
                print(content, terminator: "")
                fflush(stdout)
            }

        case .completed(let response):
            print("\n   ✅ Streaming completed!")
            if let usage = response.usage {
                print("   📊 Tokens used: \(usage.totalTokens)")
            }

        case .created:
            print("(Stream started) ", terminator: "")

        default:
            break
        }
    }
    print()

    // Step 4.2: Error handling
    print("2️⃣ Error Handling:")
    print("   Handle different types of errors gracefully")

    do {
        // Try with invalid model
        let badRequest = try ResponseRequest(
            model: "invalid-model-name",
            input: {
                user("Hello")
            }
        )

        let response = try await client.respond(to: badRequest)
        print("   🤖 Response: \(response.choices.first?.message.content ?? "None")")

    } catch LLMError.invalidModel {
        print("   ✅ Caught invalid model error")
    } catch LLMError.networkError(let message) {
        print("   ✅ Caught network error: \(message)")
    } catch {
        print("   ✅ Caught other error: \(error.localizedDescription)")
    }
    print()

    // Step 4.3: Complex configurations
    print("3️⃣ Complex Configurations:")
    print("   Combine many parameters for specific use cases")

    let creativeWritingRequest = try ResponseRequest(
        model: "gpt-4",
        config: {
            // Creativity settings
            Temperature(0.9)
            TopP(0.95)
            FrequencyPenalty(0.3)

            // Length control
            MaxOutputTokens(300)

            // Quality settings
            PresencePenalty(0.2)
        },
        input: {
            system("You are a bestselling novelist specializing in science fiction")
            user("Write the opening paragraph of a novel about AI achieving consciousness")
        }
    )

    let creativeResponse = try await client.respond(to: creativeWritingRequest)

    if let opening = creativeResponse.choices.first?.message.content {
        print("   📖 Creative writing result:")
        print("   🤖 \(opening.prefix(200))...")
    }
    print()

    // Step 4.4: Multiple response choices
    print("4️⃣ Multiple Response Choices:")
    print("   Generate several different responses to the same prompt")

    let multiChoiceRequest = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.8)  // Higher temperature for variety
        },
        input: {
            user("Suggest a creative name for a new programming language")
        }
    )

    // Note: In a real implementation, you'd set n parameter for multiple choices
    let singleResponse = try await client.respond(to: multiChoiceRequest)

    if let suggestion = singleResponse.choices.first?.message.content {
        print("   💡 Language name suggestion:")
        print("   🤖 \(suggestion)")
        print("   📝 (In production, you could request multiple choices)")
    }
    print()

    print("🎯 What You Learned:")
    print("   • Real-time streaming responses")
    print("   • Comprehensive error handling")
    print("   • Complex parameter combinations")
    print("   • Multiple response strategies")
    print()

    print("✅ Step 4 completed! You now know advanced SwiftResponsesDSL features.")
    print("🎉 Congratulations! You have completed the comprehensive tutorial!")
}

/// Main tutorial runner
func runCompleteTutorial() async {
    print("📚 SwiftResponsesDSL - Complete Step-by-Step Tutorial")
    print("===================================================")
    print("This tutorial will take you from zero to hero with SwiftResponsesDSL.")
    print("Each step builds on the previous one, introducing new concepts.\n")

    do {
        try await step1FirstRequest()
        try await step2AddingConfiguration()
        try await step3ConversationManagement()
        try await step4AdvancedFeatures()

        print("\n🎊 TUTORIAL COMPLETE!")
        print("=====================")
        print("🎯 You now know how to:")
        print("   ✅ Send basic messages to LLMs")
        print("   ✅ Control LLM behavior with parameters")
        print("   ✅ Manage multi-turn conversations")
        print("   ✅ Use advanced features like streaming")
        print("   ✅ Handle errors gracefully")
        print("   ✅ Create complex configurations")
        print()
        print("🚀 Ready to build amazing AI-powered applications!")
        print("📖 Explore other example folders for more specialized use cases.")

    } catch {
        print("❌ Tutorial failed with error:", error.localizedDescription)
        print("💡 Make sure you have:")
        print("   • Valid API credentials")
        print("   • Internet connection")
        print("   • Correct endpoint URL")
    }
}
