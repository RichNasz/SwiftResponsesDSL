//
//  EnterprisePatterns.swift
//  SwiftResponsesDSL Examples
//
//  Advanced examples showing enterprise-level patterns
//  including error recovery, monitoring, and production best practices.
//
//  Generated by AI-assisted code generation.
//  Created by Richard Naszcyniec on [Date].
//  Copyright Â© [Year] Richard Naszcyniec. All rights reserved.
//

import SwiftResponsesDSL

/// Example 1: Circuit Breaker Pattern
/// Shows how to implement resilient API interactions with automatic failure handling
func circuitBreakerExample() async throws {
    print("ðŸ”Œ Circuit Breaker Pattern")
    print("=========================")

    class CircuitBreaker {
        enum State {
            case closed    // Normal operation
            case open      // Failing, requests rejected
            case halfOpen  // Testing if service recovered
        }

        private var state: State = .closed
        private var failureCount = 0
        private let failureThreshold = 3
        private let recoveryTime: TimeInterval = 30.0
        private var lastFailureTime: Date?

        private let queue = DispatchQueue(label: "circuit-breaker")

        func execute<T>(_ operation: () async throws -> T) async throws -> T {
            try await queue.sync {
                switch state {
                case .open:
                    if let lastFailure = lastFailureTime,
                       Date().timeIntervalSince(lastFailure) > recoveryTime {
                        state = .halfOpen
                    } else {
                        throw LLMError.networkError("Circuit breaker is open - service unavailable")
                    }

                case .halfOpen, .closed:
                    break
                }
            }

            do {
                let result = try await operation()
                await recordSuccess()
                return result
            } catch {
                await recordFailure()
                throw error
            }
        }

        private func recordSuccess() async {
            await queue.sync {
                failureCount = 0
                state = .closed
            }
        }

        private func recordFailure() async {
            await queue.sync {
                failureCount += 1
                lastFailureTime = Date()

                if failureCount >= failureThreshold {
                    state = .open
                }
            }
        }
    }

    let circuitBreaker = CircuitBreaker()
    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Simulate multiple requests with potential failures
    for attempt in 1...5 {
        print("ðŸ“¡ Attempt \(attempt)")

        do {
            let result: Response = try await circuitBreaker.execute {
                let request = try ResponseRequest(
                    model: "gpt-4",
                    input: {
                        user("Hello, attempt \(attempt)")
                    }
                )
                return try await client.respond(to: request)
            }

            if let content = result.choices.first?.message.content {
                print("   âœ… Success: \(content.prefix(50))...")
            }

        } catch LLMError.networkError(let message) {
            print("   ðŸ”Œ Circuit breaker: \(message)")
        } catch {
            print("   âŒ Request failed: \(error.localizedDescription)")
        }

        // Small delay between attempts
        try await Task.sleep(nanoseconds: 500_000_000) // 0.5 seconds
    }

    print("âœ… Circuit breaker pattern demonstrated!")
}

/// Example 2: Request Retry with Exponential Backoff
/// Shows intelligent retry logic for transient failures
func retryWithBackoffExample() async throws {
    print("\nðŸ”„ Retry with Exponential Backoff")
    print("================================")

    struct RetryConfiguration {
        let maxAttempts: Int
        let baseDelay: TimeInterval
        let maxDelay: TimeInterval
        let backoffMultiplier: Double

        init(maxAttempts: Int = 3, baseDelay: TimeInterval = 1.0, maxDelay: TimeInterval = 30.0, backoffMultiplier: Double = 2.0) {
            self.maxAttempts = maxAttempts
            self.baseDelay = baseDelay
            self.maxDelay = maxDelay
            self.backoffMultiplier = backoffMultiplier
        }

        func delay(for attempt: Int) -> TimeInterval {
            let exponentialDelay = baseDelay * pow(backoffMultiplier, Double(attempt - 1))
            return min(exponentialDelay, maxDelay)
        }
    }

    func retry<T>(
        configuration: RetryConfiguration = RetryConfiguration(),
        operation: () async throws -> T
    ) async throws -> T {
        var lastError: Error?

        for attempt in 1...configuration.maxAttempts {
            do {
                return try await operation()
            } catch {
                lastError = error

                // Don't retry certain types of errors
                if case LLMError.invalidValue = error {
                    throw error
                }

                if attempt < configuration.maxAttempts {
                    let delay = configuration.delay(for: attempt)
                    print("   â³ Attempt \(attempt) failed, retrying in \(String(format: "%.1f", delay))s...")
                    try await Task.sleep(nanoseconds: UInt64(delay * 1_000_000_000))
                }
            }
        }

        throw lastError ?? LLMError.networkError("All retry attempts exhausted")
    }

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let startTime = Date()

    do {
        let response = try await retry {
            let request = try ResponseRequest(
                model: "gpt-4",
                input: {
                    user("Tell me a short joke")
                }
            )
            return try await client.respond(to: request)
        }

        let totalTime = Date().timeIntervalSince(startTime)

        if let joke = response.choices.first?.message.content {
            print("ðŸ¤– Joke (completed in \(String(format: "%.2f", totalTime))s):")
            print("   \(joke)")
        }

    } catch {
        print("âŒ All retry attempts failed:", error.localizedDescription)
    }

    print("âœ… Retry with backoff demonstrated!")
}

/// Example 3: Request Batching and Rate Limiting
/// Shows how to efficiently batch requests and respect API rate limits
func batchingAndRateLimitingExample() async throws {
    print("\nðŸ“¦ Request Batching & Rate Limiting")
    print("===================================")

    class RateLimiter {
        private var tokens: Int
        private let maxTokens: Int
        private let refillRate: Int // tokens per second
        private var lastRefill: Date
        private let queue = DispatchQueue(label: "rate-limiter")

        init(maxTokens: Int = 10, refillRate: Int = 2) {
            self.maxTokens = maxTokens
            self.refillRate = refillRate
            self.tokens = maxTokens
            self.lastRefill = Date()
        }

        func acquire() async throws {
            try await withCheckedThrowingContinuation { continuation in
                queue.async {
                    self.refillTokens()

                    if self.tokens > 0 {
                        self.tokens -= 1
                        continuation.resume()
                    } else {
                        continuation.resume(throwing: LLMError.networkError("Rate limit exceeded"))
                    }
                }
            }
        }

        private func refillTokens() {
            let now = Date()
            let timePassed = now.timeIntervalSince(lastRefill)
            let tokensToAdd = Int(timePassed * Double(refillRate))

            if tokensToAdd > 0 {
                tokens = min(maxTokens, tokens + tokensToAdd)
                lastRefill = now
            }
        }
    }

    struct RequestBatch {
        private var requests: [(id: String, request: () async throws -> Response)] = []
        private let maxBatchSize: Int
        private let rateLimiter: RateLimiter

        init(maxBatchSize: Int = 5, rateLimiter: RateLimiter = RateLimiter()) {
            self.maxBatchSize = maxBatchSize
            self.rateLimiter = rateLimiter
        }

        mutating func add(id: String, request: @escaping () async throws -> Response) {
            requests.append((id, request))
        }

        func execute() async throws -> [String: Response] {
            var results: [String: Response] = [:]

            // Process in batches
            let batches = stride(from: 0, to: requests.count, by: maxBatchSize).map {
                Array(requests[$0..<min($0 + maxBatchSize, requests.count)])
            }

            for (batchIndex, batch) in batches.enumerated() {
                print("ðŸ“¦ Processing batch \(batchIndex + 1)/\(batches.count) (\(batch.count) requests)")

                try await withThrowingTaskGroup(of: (String, Response).self) { group in
                    for (id, request) in batch {
                        group.addTask {
                            // Acquire rate limit token
                            try await rateLimiter.acquire()
                            let response = try await request()
                            return (id, response)
                        }
                    }

                    // Collect results
                    for try await (id, response) in group {
                        results[id] = response
                    }
                }

                // Small delay between batches
                if batchIndex < batches.count - 1 {
                    try await Task.sleep(nanoseconds: 1_000_000_000) // 1 second
                }
            }

            return results
        }
    }

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")
    let rateLimiter = RateLimiter(maxTokens: 3, refillRate: 1) // 3 requests, refill 1 per second
    var batch = RequestBatch(maxBatchSize: 2, rateLimiter: rateLimiter)

    // Add multiple requests to the batch
    let prompts = [
        "request1": "Explain recursion in one sentence",
        "request2": "What is the capital of France?",
        "request3": "Write a haiku about coding",
        "request4": "What is 15 + 27?",
        "request5": "Name three programming languages"
    ]

    for (id, prompt) in prompts {
        batch.add(id: id) {
            let request = try ResponseRequest(
                model: "gpt-4",
                input: {
                    user(prompt)
                }
            )
            return try await client.respond(to: request)
        }
    }

    print("ðŸš€ Executing batch of \(prompts.count) requests...")
    let results = try await batch.execute()

    print("\nðŸ“Š Batch Results:")
    for (id, response) in results.sorted(by: { $0.key < $1.key }) {
        if let content = response.choices.first?.message.content {
            print("   \(id): \(content.prefix(50))...")
        }
    }

    print("âœ… Batching and rate limiting demonstrated!")
}

/// Example 4: Comprehensive Monitoring and Logging
/// Shows enterprise-grade monitoring and observability patterns
func monitoringAndLoggingExample() async throws {
    print("\nðŸ“Š Monitoring & Logging")
    print("=======================")

    struct MetricsCollector {
        private var metrics: [String: Any] = [:]
        private let queue = DispatchQueue(label: "metrics-collector")

        func record(name: String, value: Any) {
            queue.sync {
                metrics[name] = value
            }
        }

        func increment(name: String, by amount: Int = 1) {
            queue.sync {
                let current = (metrics[name] as? Int) ?? 0
                metrics[name] = current + amount
            }
        }

        func getMetrics() -> [String: Any] {
            queue.sync { metrics }
        }

        func reset() {
            queue.sync { metrics.removeAll() }
        }
    }

    struct RequestLogger {
        enum LogLevel: String {
            case debug, info, warning, error
        }

        static func log(level: LogLevel, message: String, metadata: [String: Any] = [:]) {
            let timestamp = ISO8601DateFormatter().string(from: Date())
            var logMessage = "[\(timestamp)] [\(level.rawValue.uppercased())] \(message)"

            if !metadata.isEmpty {
                logMessage += " | Metadata: \(metadata)"
            }

            print(logMessage)
        }
    }

    let metrics = MetricsCollector()
    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Monitor multiple requests
    let testPrompts = [
        "Hello, how are you?",
        "What's the weather like?",
        "Tell me a joke",
        "Explain quantum physics briefly"
    ]

    print("ðŸ“ˆ Starting monitored requests...")

    for (index, prompt) in testPrompts.enumerated() {
        let requestId = "req-\(index + 1)"
        let startTime = Date()

        RequestLogger.log(level: .info, message: "Starting request", metadata: [
            "requestId": requestId,
            "prompt": prompt
        ])

        do {
            let request = try ResponseRequest(
                model: "gpt-4",
                config: {
                    Temperature(0.7)
                    MaxOutputTokens(100)
                },
                input: {
                    user(prompt)
                }
            )

            let response = try await client.respond(to: request)
            let duration = Date().timeIntervalSince(startTime)

            metrics.increment("requests.success")
            metrics.record(name: "request.\(requestId).duration", value: duration)

            if let content = response.choices.first?.message.content,
               let usage = response.usage {

                metrics.record(name: "request.\(requestId).tokens", value: usage.totalTokens)

                RequestLogger.log(level: .info, message: "Request completed", metadata: [
                    "requestId": requestId,
                    "duration": String(format: "%.3f", duration),
                    "tokens": usage.totalTokens,
                    "responseLength": content.count
                ])
            }

        } catch {
            let duration = Date().timeIntervalSince(startTime)
            metrics.increment("requests.error")

            RequestLogger.log(level: .error, message: "Request failed", metadata: [
                "requestId": requestId,
                "error": error.localizedDescription,
                "duration": String(format: "%.3f", duration)
            ])
        }
    }

    // Display final metrics
    let finalMetrics = metrics.getMetrics()
    print("\nðŸ“Š Final Metrics:")
    for (key, value) in finalMetrics.sorted(by: { $0.key < $1.key }) {
        print("   \(key): \(value)")
    }

    print("âœ… Monitoring and logging demonstrated!")
}

/// Main function to run all enterprise pattern examples
func runEnterpriseExamples() async {
    print("ðŸ¢ SwiftResponsesDSL - Enterprise Patterns")
    print("========================================")
    print("Learn enterprise-grade patterns for production deployments.\n")

    do {
        try await circuitBreakerExample()
        try await retryWithBackoffExample()
        try await batchingAndRateLimitingExample()
        try await monitoringAndLoggingExample()

        print("\nðŸŽ‰ All enterprise examples completed!")
        print("ðŸ’¡ These patterns ensure reliable, scalable LLM integrations!")

    } catch {
        print("âŒ Example failed with error:", error.localizedDescription)
    }
}
