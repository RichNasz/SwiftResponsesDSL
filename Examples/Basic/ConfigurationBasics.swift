//
//  ConfigurationBasics.swift
//  SwiftResponsesDSL Examples
//
//  Basic examples showing how to configure LLM parameters
//  for different types of responses.
//
//  Generated by AI-assisted code generation.
//  Created by Richard Naszcyniec on [Date].
//  Copyright ¬© [Year] Richard Naszcyniec. All rights reserved.
//

import SwiftResponsesDSL

/// Example 1: Temperature Control
/// Shows how temperature affects response creativity
func temperatureExample() async throws {
    print("üå°Ô∏è  Temperature Control Example")
    print("================================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let temperatures = [0.0, 0.7, 1.5] // Low to high creativity

    for temp in temperatures {
        print("üìù Temperature: \(temp)")

        let request = try ResponseRequest(
            model: "gpt-4",
            config: {
                Temperature(temp)
                MaxOutputTokens(50) // Keep responses short for comparison
            },
            input: {
                system("You are a creative writer")
                user("Write a one-sentence story about a cat")
            }
        )

        let response = try await client.respond(to: request)
        if let content = response.choices.first?.message.content {
            print("   ü§ñ Story:", content.trimmingCharacters(in: .whitespacesAndNewlines))
        }
        print()
    }

    print("‚úÖ Temperature control demonstrated!")
}

/// Example 2: Response Length Control
/// Shows how to limit or expand response lengths
func responseLengthExample() async throws {
    print("\nüìè Response Length Control Example")
    print("==================================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let lengths = [10, 50, 200] // Short to long responses

    for maxTokens in lengths {
        print("üìù Max Tokens: \(maxTokens)")

        let request = try ResponseRequest(
            model: "gpt-4",
            config: {
                Temperature(0.7)
                MaxOutputTokens(maxTokens)
            },
            input: {
                user("Explain what artificial intelligence is")
            }
        )

        let response = try await client.respond(to: request)
        if let content = response.choices.first?.message.content {
            let wordCount = content.split(separator: " ").count
            print("   ü§ñ Response (\(wordCount) words):",
                  content.prefix(100) + (content.count > 100 ? "..." : ""))
        }
        print()
    }

    print("‚úÖ Response length control demonstrated!")
}

/// Example 3: Multiple Parameters Together
/// Shows how to combine different configuration parameters
func combinedParametersExample() async throws {
    print("\n‚öôÔ∏è  Combined Parameters Example")
    print("==============================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Creative writing with specific constraints
    let request = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.9)        // High creativity
            MaxOutputTokens(150)    // Medium length
            TopP(0.9)              // Nucleus sampling
            FrequencyPenalty(0.3)   // Reduce repetition
            PresencePenalty(0.1)    // Encourage new topics
        },
        input: {
            system("You are a creative storyteller specializing in science fiction")
            user("Write a short story about a robot learning to paint")
        }
    )

    let response = try await client.respond(to: request)
    if let content = response.choices.first?.message.content {
        print("ü§ñ Creative Story:")
        print(content)
    }

    print("‚úÖ Combined parameters demonstrated!")
}

/// Example 4: Parameter Validation
/// Shows what happens when you provide invalid parameter values
func parameterValidationExample() async {
    print("\n‚úÖ Parameter Validation Example")
    print("===============================")

    do {
        // This will succeed - valid parameters
        let validRequest = try ResponseRequest(
            model: "gpt-4",
            config: {
                Temperature(0.7)      // Valid: 0.0 to 2.0
                MaxOutputTokens(100)  // Valid: 1 to 4096
            },
            input: {
                user("Hello")
            }
        )
        print("‚úÖ Valid parameters accepted")

    } catch LLMError.invalidValue(let message) {
        print("‚ùå Parameter validation failed:", message)
    } catch {
        print("‚ùå Unexpected error:", error.localizedDescription)
    }

    do {
        // This will fail - invalid temperature
        let invalidRequest = try ResponseRequest(
            model: "gpt-4",
            config: {
                Temperature(5.0)      // Invalid: outside 0.0 to 2.0 range
                MaxOutputTokens(100)
            },
            input: {
                user("Hello")
            }
        )
        print("‚úÖ This shouldn't print")

    } catch LLMError.invalidValue(let message) {
        print("‚ùå Expected validation error:", message)
    } catch {
        print("‚ùå Unexpected error:", error.localizedDescription)
    }

    print("‚úÖ Parameter validation demonstrated!")
}

/// Example 5: Default vs Custom Configuration
/// Shows the difference between using defaults and custom settings
func defaultsVsCustomExample() async throws {
    print("\nüîÑ Default vs Custom Configuration")
    print("==================================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Example 1: Using defaults (minimal configuration)
    print("üìù Using Default Settings:")
    let defaultRequest = try ResponseRequest(
        model: "gpt-4",
        input: {
            user("Write a haiku about programming")
        }
    )

    let defaultResponse = try await client.respond(to: defaultRequest)
    if let content = defaultResponse.choices.first?.message.content {
        print("   ü§ñ Default:", content.trimmingCharacters(in: .whitespacesAndNewlines))
    }

    // Example 2: Using custom configuration
    print("\nüìù Using Custom Settings:")
    let customRequest = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.3)        // More focused/consistent
            MaxOutputTokens(30)     // Shorter response
        },
        input: {
            user("Write a haiku about programming")
        }
    )

    let customResponse = try await client.respond(to: customRequest)
    if let content = customResponse.choices.first?.message.content {
        print("   ü§ñ Custom:", content.trimmingCharacters(in: .whitespacesAndNewlines))
    }

    print("‚úÖ Configuration comparison completed!")
}

/// Main function to run all configuration examples
func runConfigurationExamples() async {
    print("‚öôÔ∏è  SwiftResponsesDSL - Configuration Basics")
    print("===========================================")
    print("Learn how to control LLM behavior with configuration parameters.\n")

    do {
        try await temperatureExample()
        try await responseLengthExample()
        try await combinedParametersExample()
        await parameterValidationExample()
        try await defaultsVsCustomExample()

        print("\nüéâ All configuration examples completed!")
        print("üí° Next: Explore Intermediate examples for advanced features!")

    } catch {
        print("‚ùå Example failed with error:", error.localizedDescription)
    }
}
