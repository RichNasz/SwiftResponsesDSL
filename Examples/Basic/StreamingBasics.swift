//
//  StreamingBasics.swift
//  SwiftResponsesDSL Examples
//
//  Basic examples showing how to use streaming responses
//  for real-time interactions with LLMs.
//
//  Generated by AI-assisted code generation.
//  Created by Richard Naszcyniec on [Date].
//  Copyright © [Year] Richard Naszcyniec. All rights reserved.
//

import SwiftResponsesDSL

/// Example 1: Basic Streaming
/// Shows the simplest way to use streaming responses
func basicStreamingExample() async throws {
    print("🌊 Basic Streaming Example")
    print("==========================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let request = try ResponseRequest(
        model: "gpt-4",
        config: {
            StreamOptions(["include_usage": true])
        },
        input: {
            user("Write a short poem about the ocean")
        }
    )

    print("📝 Streaming response:")
    print("🤖 ", terminator: "")

    let stream = client.stream(request: request)
    var wordCount = 0

    for try await event in stream {
        switch event {
        case .created:
            print("(Stream started)", terminator: " ")

        case .inProgress:
            // Could show a progress indicator here
            break

        case .outputItemAdded(let item):
            if case .message(let message) = item,
               let content = message.content {
                // Print content as it arrives
                print(content, terminator: "")
                fflush(stdout) // Force immediate output
                wordCount += content.split(separator: " ").count
            }

        case .completed(let response):
            print("\n✅ Stream completed!")
            print("📊 Total words received: \(wordCount)")

            // Show usage statistics if available
            if let usage = response.usage {
                print("📈 Token usage - Input: \(usage.inputTokens), Output: \(usage.outputTokens)")
            }

        case .unknown(let type, _):
            print("\n⚠️  Unknown event type: \(type)", terminator: " ")
        }
    }

    print("\n✅ Basic streaming completed!")
}

/// Example 2: Streaming with Progress Indicators
/// Shows how to provide user feedback during streaming
func streamingWithProgressExample() async throws {
    print("\n📊 Streaming with Progress Example")
    print("==================================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let request = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.7)
            MaxOutputTokens(100)
            StreamOptions(["include_usage": true])
        },
        input: {
            user("Explain quantum computing in simple terms")
        }
    )

    print("🤖 Assistant is thinking...")
    var dots = 0
    var totalContent = ""

    let stream = client.stream(request: request)

    for try await event in stream {
        switch event {
        case .created:
            print("✅ Connection established")

        case .inProgress:
            // Show thinking animation
            dots = (dots + 1) % 4
            let animation = String(repeating: ".", count: dots) + String(repeating: " ", count: 3 - dots)
            print("\r🤔 Thinking\(animation)", terminator: "")
            fflush(stdout)

        case .outputItemAdded(let item):
            if case .message(let message) = item,
               let content = message.content {
                totalContent += content
                // Clear the thinking line and show content
                print("\r🤖 \(totalContent)", terminator: "")
                fflush(stdout)
            }

        case .completed(let response):
            print("\n✅ Response completed!")

            if let usage = response.usage {
                let totalTokens = usage.totalTokens
                let estimatedWords = totalContent.split(separator: " ").count
                print("📊 Statistics:")
                print("   • Tokens used: \(totalTokens)")
                print("   • Estimated words: \(estimatedWords)")
                print("   • Avg tokens per word: \(Double(totalTokens) / Double(estimatedWords))")
            }

        case .unknown(let type, _):
            print("\n⚠️  Received unknown event: \(type)")
        }
    }

    print("✅ Progress streaming completed!")
}

/// Example 3: Streaming with Cancellation
/// Shows how to cancel a streaming response mid-way
func streamingWithCancellationExample() async throws {
    print("\n🛑 Streaming with Cancellation Example")
    print("======================================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let request = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.8)
            MaxOutputTokens(200) // Longer response to demonstrate cancellation
        },
        input: {
            user("Write a detailed story about space exploration")
        }
    )

    print("🚀 Starting streaming response (will cancel after 3 seconds)...")

    let stream = client.stream(request: request)
    var startTime = Date()
    var contentReceived = ""

    do {
        for try await event in stream {
            switch event {
            case .outputItemAdded(let item):
                if case .message(let message) = item,
                   let content = message.content {
                    contentReceived += content
                    print(content, terminator: "")
                    fflush(stdout)

                    // Cancel after 3 seconds
                    if Date().timeIntervalSince(startTime) > 3.0 {
                        print("\n\n🛑 Cancelling stream after 3 seconds...")
                        break
                    }
                }

            case .completed:
                print("\n✅ Stream completed naturally")

            default:
                break
            }
        }
    } catch {
        print("\n❌ Stream error:", error.localizedDescription)
    }

    let wordCount = contentReceived.split(separator: " ").count
    print("📊 Partial response received: \(wordCount) words")
    print("✅ Cancellation example completed!")
}

/// Example 4: Comparing Streaming vs Non-Streaming
/// Shows the difference in user experience between the two approaches
func streamingVsNonStreamingExample() async throws {
    print("\n⚡ Streaming vs Non-Streaming Comparison")
    print("========================================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let prompt = "Write a short explanation of how photosynthesis works"

    // First, try non-streaming
    print("📝 Non-Streaming Approach:")
    print("   (Waiting for complete response...)")

    let startTime = Date()

    let nonStreamingRequest = try ResponseRequest(
        model: "gpt-4",
        input: {
            user(prompt)
        }
    )

    let nonStreamingResponse = try await client.respond(to: nonStreamingRequest)
    let nonStreamingTime = Date().timeIntervalSince(startTime)

    if let content = nonStreamingResponse.choices.first?.message.content {
        print("   ✅ Complete response received in \(String(format: "%.2f", nonStreamingTime))s")
        print("   🤖 \(content.prefix(100))...")
    }

    // Now try streaming
    print("\n🌊 Streaming Approach:")
    print("   (Receiving response in real-time...)")

    let streamingStartTime = Date()
    var streamingContent = ""

    let streamingRequest = try ResponseRequest(
        model: "gpt-4",
        config: {
            StreamOptions(["include_usage": true])
        },
        input: {
            user(prompt)
        }
    )

    let stream = client.stream(request: streamingRequest)

    for try await event in stream {
        if case .outputItemAdded(let item) = event,
           case .message(let message) = item,
           let content = message.content {
            streamingContent += content
            // Show first part of streaming response
            if streamingContent.count <= 100 {
                print("   📝 \(streamingContent)", terminator: "")
                fflush(stdout)
            }
        }
    }

    let streamingTime = Date().timeIntervalSince(streamingStartTime)
    print("\n   ✅ Streaming completed in \(String(format: "%.2f", streamingTime))s")

    // Compare the results
    print("\n📊 Comparison:")
    print("   • Non-streaming: \(String(format: "%.2f", nonStreamingTime))s (wait for complete)")
    print("   • Streaming: \(String(format: "%.2f", streamingTime))s (real-time feedback)")
    print("   • User Experience: Streaming provides immediate feedback!")

    print("✅ Comparison completed!")
}

/// Example 5: Handling Streaming Errors
/// Shows how to handle errors that can occur during streaming
func streamingErrorHandlingExample() async {
    print("\n🚨 Streaming Error Handling Example")
    print("===================================")

    do {
        // Try with an invalid endpoint to demonstrate error handling
        let client = try LLMClient(baseURLString: "https://invalid-endpoint.com")

        let request = try ResponseRequest(
            model: "gpt-4",
            input: {
                user("Hello")
            }
        )

        let stream = client.stream(request: request)

        for try await event in stream {
            print("📡 Received event:", event)
        }

    } catch LLMError.networkError(let message) {
        print("❌ Network Error:", message)
        print("💡 In a real app, you might:")
        print("   • Show a retry button")
        print("   • Fall back to non-streaming")
        print("   • Show an offline message")

    } catch LLMError.invalidURL {
        print("❌ Invalid URL Error")
        print("💡 Check your API endpoint configuration")

    } catch {
        print("❌ Unexpected Error:", error.localizedDescription)
        print("💡 Log this error for debugging")
    }

    print("✅ Error handling demonstrated!")
}

/// Main function to run all streaming examples
func runStreamingExamples() async {
    print("🌊 SwiftResponsesDSL - Streaming Basics")
    print("=======================================")
    print("Learn how to use streaming responses for real-time interactions.\n")

    do {
        try await basicStreamingExample()
        try await streamingWithProgressExample()
        try await streamingWithCancellationExample()
        try await streamingVsNonStreamingExample()
        await streamingErrorHandlingExample()

        print("\n🎉 All streaming examples completed!")
        print("💡 Next: Try the Intermediate examples for more advanced patterns!")

    } catch {
        print("❌ Example failed with error:", error.localizedDescription)
    }
}
