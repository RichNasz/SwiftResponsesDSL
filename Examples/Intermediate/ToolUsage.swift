//
//  ToolUsage.swift
//  SwiftResponsesDSL Examples
//
//  Intermediate examples showing how to use tools and function calling
//  to extend LLM capabilities with external data and actions.
//
//  Generated by AI-assisted code generation.
//  Created by Richard Naszcyniec on [Date].
//  Copyright ¬© [Year] Richard Naszcyniec. All rights reserved.
//

import SwiftResponsesDSL

/// Example 1: Basic Tool Usage
/// Shows how to define and use a simple tool
func basicToolExample() async throws {
    print("üîß Basic Tool Usage")
    print("===================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Define a simple calculator tool
    let calculatorTool = Tool(
        type: .function,
        function: Tool.Function(
            name: "calculate",
            description: "Perform mathematical calculations",
            parameters: .object([
                "expression": .string(description: "Mathematical expression to evaluate")
            ])
        )
    )

    let request = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.1)  // Precise calculations
        },
        input: {
            user("What's 15 * 23 + 7?")
        },
        tools: [calculatorTool]
    )

    print("üßÆ Asking AI to calculate...")
    let response = try await client.respond(to: request)

    // Check if the AI wants to use a tool
    if let toolCalls = response.outputItems?.filter({ $0.type == .toolCall }),
       let toolCall = toolCalls.first {

        print("ü§ñ AI wants to use tool: \(toolCall.toolCall?.function?.name ?? "Unknown")")

        // In a real implementation, you would execute the tool here
        // For this example, we'll simulate the tool response
        print("üîß Tool execution would happen here...")
        print("üìä Tool result: 15 * 23 + 7 = 352")
    } else if let content = response.choices.first?.message.content {
        print("ü§ñ Direct answer: \(content)")
    }

    print("‚úÖ Basic tool usage demonstrated!")
}

/// Example 2: Weather Tool Integration
/// Shows how to create a weather lookup tool
func weatherToolExample() async throws {
    print("\nüå§Ô∏è  Weather Tool Integration")
    print("============================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Define a weather lookup tool
    let weatherTool = Tool(
        type: .function,
        function: Tool.Function(
            name: "get_weather",
            description: "Get current weather for a location",
            parameters: .object([
                "location": .string(description: "City name or location"),
                "unit": .string(
                    description: "Temperature unit (celsius or fahrenheit)",
                    enum: ["celsius", "fahrenheit"]
                )
            ])
        )
    )

    let request = try ResponseRequest(
        model: "gpt-4",
        input: {
            user("What's the weather like in San Francisco?")
        },
        tools: [weatherTool]
    )

    print("üå§Ô∏è Checking weather...")
    let response = try await client.respond(to: request)

    if let toolCalls = response.outputItems?.filter({ $0.type == .toolCall }) {
        print("üì° AI requested \(toolCalls.count) tool call(s)")

        for toolCall in toolCalls {
            if let function = toolCall.toolCall?.function {
                print("üîß Tool: \(function.name)")
                print("üìù Arguments: \(function.arguments ?? "{}")")

                // Simulate tool execution
                if function.name == "get_weather" {
                    print("üå§Ô∏è Weather API would be called here...")
                    print("üìä Simulated result: 72¬∞F, Partly Cloudy")
                }
            }
        }
    } else if let content = response.choices.first?.message.content {
        print("ü§ñ Direct response: \(content)")
    }

    print("‚úÖ Weather tool integration demonstrated!")
}

/// Example 3: Multiple Tools with Selection
/// Shows how AI can choose between multiple available tools
func multipleToolsExample() async throws {
    print("\nüõ†Ô∏è  Multiple Tools Example")
    print("=========================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Define multiple tools
    let tools = [
        Tool(
            type: .function,
            function: Tool.Function(
                name: "search_web",
                description: "Search the web for information",
                parameters: .object([
                    "query": .string(description: "Search query")
                ])
            )
        ),
        Tool(
            type: .function,
            function: Tool.Function(
                name: "calculate",
                description: "Perform mathematical calculations",
                parameters: .object([
                    "expression": .string(description: "Math expression")
                ])
            )
        ),
        Tool(
            type: .function,
            function: Tool.Function(
                name: "get_weather",
                description: "Get weather information",
                parameters: .object([
                    "location": .string(description: "Location name")
                ])
            )
        )
    ]

    let testQueries = [
        "What is the square root of 144?",
        "What's the weather in London?",
        "Who won the Nobel Prize in Physics in 2023?"
    ]

    for query in testQueries {
        print("\nüìù Query: \(query)")

        let request = try ResponseRequest(
            model: "gpt-4",
            input: {
                user(query)
            },
            tools: tools
        )

        let response = try await client.respond(to: request)

        if let toolCalls = response.outputItems?.filter({ $0.type == .toolCall }) {
            for toolCall in toolCalls {
                if let function = toolCall.toolCall?.function {
                    print("   üîß Selected tool: \(function.name)")
                }
            }
        } else {
            print("   ü§ñ No tool needed - direct response")
        }
    }

    print("‚úÖ Multiple tools selection demonstrated!")
}

/// Example 4: Tool Results Processing
/// Shows how to handle tool results and continue conversations
func toolResultsProcessingExample() async throws {
    print("\nüîÑ Tool Results Processing")
    print("==========================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Start with a tool-enabled conversation
    var conversation = ResponseConversation()

    let searchTool = Tool(
        type: .function,
        function: Tool.Function(
            name: "web_search",
            description: "Search for current information on the web",
            parameters: .object([
                "query": .string(description: "Search query")
            ])
        )
    )

    conversation.append(user: "What's the current population of Tokyo?")

    // First request - AI might call the tool
    let firstRequest = try ResponseRequest(
        model: "gpt-4",
        input: conversation.messages,
        tools: [searchTool]
    )

    let firstResponse = try await client.respond(to: firstRequest)

    if let toolCalls = firstResponse.outputItems?.filter({ $0.type == .toolCall }),
       let toolCall = toolCalls.first {

        print("ü§ñ AI requested tool: \(toolCall.toolCall?.function?.name ?? "Unknown")")

        // Simulate tool execution and add result to conversation
        conversation.append(tool: ToolMessage(
            toolCallId: toolCall.toolCall?.id ?? "",
            content: "According to recent data, Tokyo's population is approximately 13.96 million people (as of 2023)."
        ))

        // Continue the conversation with tool results
        let followUpRequest = try ResponseRequest(
            model: "gpt-4",
            input: conversation.messages
        )

        let followUpResponse = try await client.respond(to: followUpRequest)

        if let finalAnswer = followUpResponse.choices.first?.message.content {
            print("ü§ñ Final answer with context: \(finalAnswer)")
        }

    } else if let directAnswer = firstResponse.choices.first?.message.content {
        print("ü§ñ Direct answer: \(directAnswer)")
    }

    print("‚úÖ Tool results processing demonstrated!")
}

/// Example 5: Complex Tool Chains
/// Shows how multiple tools can be used in sequence
func complexToolChainsExample() async throws {
    print("\nüîó Complex Tool Chains")
    print("=====================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Define a comprehensive set of tools
    let tools = [
        Tool(type: .function, function: Tool.Function(
            name: "search_restaurants",
            description: "Find restaurants in a location",
            parameters: .object([
                "location": .string(description: "City or area"),
                "cuisine": .string(description: "Type of cuisine"),
                "price_range": .string(description: "Budget level", enum: ["$", "$$", "$$$"])
            ])
        )),
        Tool(type: .function, function: Tool.Function(
            name: "get_directions",
            description: "Get directions between locations",
            parameters: .object([
                "origin": .string(description: "Starting location"),
                "destination": .string(description: "Destination location"),
                "transport_mode": .string(description: "Transportation method", enum: ["driving", "walking", "transit"])
            ])
        )),
        Tool(type: .function, function: Tool.Function(
            name: "check_availability",
            description: "Check if a restaurant has available reservations",
            parameters: .object([
                "restaurant_id": .string(description: "Restaurant identifier"),
                "date": .string(description: "Date for reservation"),
                "time": .string(description: "Time for reservation"),
                "party_size": .integer(description: "Number of people", minimum: 1, maximum: 20)
            ])
        ))
    ]

    let complexQuery = """
    I want to find an Italian restaurant in downtown San Francisco for tonight at 7 PM.
    I have a budget of $$ (moderate) and there will be 4 people.
    Please find options and check availability.
    """

    print("üçù Complex restaurant planning request...")

    let request = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.2)        // Precise planning
            MaxOutputTokens(400)    // Allow detailed response
        },
        input: {
            user(complexQuery)
        },
        tools: tools
    )

    let response = try await client.respond(to: request)

    if let toolCalls = response.outputItems?.filter({ $0.type == .toolCall }) {
        print("üîß AI orchestrated \(toolCalls.count) tool calls:")

        for (index, toolCall) in toolCalls.enumerated() {
            if let function = toolCall.toolCall?.function {
                print("   \(index + 1). \(function.name)")
                print("      Args: \(function.arguments ?? "{}")")
            }
        }

        print("üìä This demonstrates how AI can:")
        print("   ‚Ä¢ Break down complex requests")
        print("   ‚Ä¢ Use multiple tools in sequence")
        print("   ‚Ä¢ Coordinate information gathering")

    } else if let directResponse = response.choices.first?.message.content {
        print("ü§ñ Direct planning response:")
        print("   \(directResponse)")
    }

    print("‚úÖ Complex tool chains demonstrated!")
}

/// Example 6: Error Handling with Tools
/// Shows how to handle tool execution errors gracefully
func toolErrorHandlingExample() async {
    print("\nüö® Tool Error Handling")
    print("=====================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Tool that might fail
    let unreliableTool = Tool(
        type: .function,
        function: Tool.Function(
            name: "unreliable_service",
            description: "A service that might be unavailable",
            parameters: .object([
                "request": .string(description: "Request to process")
            ])
        )
    )

    let request = try ResponseRequest(
        model: "gpt-4",
        input: {
            user("Please process this request using the unreliable service")
        },
        tools: [unreliableTool]
    )

    do {
        let response = try await client.respond(to: request)

        if let toolCalls = response.outputItems?.filter({ $0.type == .toolCall }) {
            print("üîß Tool call requested")

            // Simulate tool execution failure
            let toolError = ToolMessage(
                toolCallId: toolCalls.first?.toolCall?.id ?? "",
                content: "Error: Service temporarily unavailable. Please try again later."
            )

            print("‚ùå Tool execution failed: \(toolError.content)")

            // Continue conversation with error information
            var errorConversation = ResponseConversation()
            errorConversation.append(user: "Please process this request using the unreliable service")
            errorConversation.append(message: response.choices.first?.message ?? UserMessage(text: ""))
            errorConversation.append(tool: toolError)

            let recoveryRequest = try ResponseRequest(
                model: "gpt-4",
                input: errorConversation.messages
            )

            let recoveryResponse = try await client.respond(to: recoveryRequest)

            if let recovery = recoveryResponse.choices.first?.message.content {
                print("ü§ñ AI handled error gracefully: \(recovery)")
            }
        }

    } catch let error as LLMError {
        print("‚ùå Request failed with LLMError: \(error.localizedDescription)")
    } catch {
        print("‚ùå Unexpected error: \(error.localizedDescription)")
    }

    print("‚úÖ Tool error handling demonstrated!")
}

/// Main function to run all tool usage examples
func runToolExamples() async {
    print("üîß SwiftResponsesDSL - Tool Usage")
    print("===============================")
    print("Learn how to extend LLM capabilities with external tools and functions.\n")

    do {
        try await basicToolExample()
        try await weatherToolExample()
        try await multipleToolsExample()
        try await toolResultsProcessingExample()
        try await complexToolChainsExample()
        await toolErrorHandlingExample()

        print("\nüéâ All tool usage examples completed!")
        print("üí° Tools allow LLMs to access external data and perform actions!")

    } catch {
        print("‚ùå Example failed with error:", error.localizedDescription)
        print("üí° Note: Tool calling requires compatible models (like gpt-4)")
    }
}
