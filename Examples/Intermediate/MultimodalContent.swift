//
//  MultimodalContent.swift
//  SwiftResponsesDSL Examples
//
//  Intermediate examples showing how to work with multimodal content
//  including images, files, and structured data.
//
//  Generated by AI-assisted code generation.
//  Created by Richard Naszcyniec on [Date].
//  Copyright Â© [Year] Richard Naszcyniec. All rights reserved.
//

import SwiftResponsesDSL

/// Example 1: Image Analysis
/// Shows how to send images to LLMs for analysis
func imageAnalysisExample() async throws {
    print("ðŸ–¼ï¸  Image Analysis Example")
    print("========================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Example image URL (you would use your own image)
    let imageUrl = "https://picsum.photos/800/600?random=1"

    let request = try ResponseRequest(
        model: "gpt-4-vision-preview", // Vision-capable model
        input: {
            user([
                .text("What's in this image? Describe it in detail."),
                .imageUrl(url: imageUrl, detail: .high)
            ])
        }
    )

    print("ðŸ“¸ Analyzing image...")
    let response = try await client.respond(to: request)

    if let analysis = response.choices.first?.message.content {
        print("ðŸ¤– Image Analysis:")
        print("   \(analysis)")
    }

    print("âœ… Image analysis completed!")
}

/// Example 2: Comparing Image Detail Levels
/// Shows how different detail levels affect analysis quality and token usage
func imageDetailComparisonExample() async throws {
    print("\nðŸ” Image Detail Levels Comparison")
    print("=================================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let imageUrl = "https://picsum.photos/800/600?random=2"
    let prompt = "Count how many distinct objects you can see in this image."

    let detailLevels: [ImageDetail] = [.low, .high]

    for detail in detailLevels {
        print("\nðŸ“Š Detail Level: \(detail)")

        let request = try ResponseRequest(
            model: "gpt-4-vision-preview",
            input: {
                user([
                    .text(prompt),
                    .imageUrl(url: imageUrl, detail: detail)
                ])
            }
        )

        let response = try await client.respond(to: request)

        if let result = response.choices.first?.message.content {
            print("   ðŸ¤– Analysis: \(result)")
        }

        // Show usage comparison
        if let usage = response.usage {
            print("   ðŸ“ˆ Tokens used: \(usage.totalTokens) (Input: \(usage.inputTokens), Output: \(usage.outputTokens))")
        }
    }

    print("âœ… Detail comparison completed!")
}

/// Example 3: Multiple Images in One Request
/// Shows how to analyze multiple images simultaneously
func multipleImagesExample() async throws {
    print("\nðŸ“š Multiple Images Analysis")
    print("===========================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let imageUrls = [
        "https://picsum.photos/400/300?random=1",
        "https://picsum.photos/400/300?random=2",
        "https://picsum.photos/400/300?random=3"
    ]

    var contentParts: [ContentPart] = [.text("Compare these three images. What themes or patterns do you notice?")]

    // Add all images to the content
    for url in imageUrls {
        contentParts.append(.imageUrl(url: url, detail: .high))
    }

    let request = try ResponseRequest(
        model: "gpt-4-vision-preview",
        input: {
            user(contentParts)
        }
    )

    print("ðŸŽ¨ Analyzing multiple images...")
    let response = try await client.respond(to: request)

    if let comparison = response.choices.first?.message.content {
        print("ðŸ¤– Multi-Image Analysis:")
        print("   \(comparison)")
    }

    print("âœ… Multiple images analysis completed!")
}

/// Example 4: Image with Text Context
/// Shows how to provide text context alongside images
func imageWithContextExample() async throws {
    print("\nðŸ“ Image with Text Context")
    print("==========================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let imageUrl = "https://picsum.photos/600/400?random=4"

    let request = try ResponseRequest(
        model: "gpt-4-vision-preview",
        input: {
            user([
                .text("I'm planning a birthday party and found this venue. Help me analyze if it's suitable:"),
                .text("- Budget: $2000 for 20 guests"),
                .text("- Theme: Garden party"),
                .text("- Requirements: Indoor backup plan, parking, catering facilities"),
                .imageUrl(url: imageUrl, detail: .high),
                .text("What do you think? Any recommendations for improvements?")
            ])
        }
    )

    print("ðŸŽ‰ Venue analysis for birthday party...")
    let response = try await client.respond(to: request)

    if let analysis = response.choices.first?.message.content {
        print("ðŸ¤– Venue Analysis:")
        print("   \(analysis)")
    }

    print("âœ… Contextual image analysis completed!")
}

/// Example 5: File Content Analysis
/// Shows how to work with file-based content
func fileContentExample() async throws {
    print("\nðŸ“„ File Content Analysis")
    print("========================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Example: Analyzing a code file (you would upload a real file)
    let codeSnippet = """
    func fibonacci(_ n: Int) -> Int {
        if n <= 1 { return n }
        return fibonacci(n-1) + fibonacci(n-2)
    }

    // This is an inefficient implementation
    // Can you suggest improvements?
    """

    let request = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.3)        // More analytical
            MaxOutputTokens(200)
        },
        input: {
            system("You are a senior software engineer specializing in algorithm optimization.")
            user([
                .text("Please analyze this code snippet and suggest improvements:"),
                .text("```swift\n\(codeSnippet)\n```"),
                .text("Focus on performance, readability, and best practices.")
            ])
        }
    )

    print("ðŸ’» Analyzing code snippet...")
    let response = try await client.respond(to: request)

    if let analysis = response.choices.first?.message.content {
        print("ðŸ¤– Code Analysis:")
        print("   \(analysis)")
    }

    print("âœ… File content analysis completed!")
}

/// Example 6: Structured Data Analysis
/// Shows how to work with JSON or structured data
func structuredDataExample() async throws {
    print("\nðŸ“Š Structured Data Analysis")
    print("===========================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let jsonData = """
    {
        "sales": [
            {"month": "Jan", "revenue": 15000, "customers": 120},
            {"month": "Feb", "revenue": 18000, "customers": 135},
            {"month": "Mar", "revenue": 22000, "customers": 150},
            {"month": "Apr", "revenue": 19000, "customers": 142}
        ],
        "expenses": {
            "marketing": 5000,
            "operations": 8000,
            "development": 12000
        }
    }
    """

    let request = try ResponseRequest(
        model: "gpt-4",
        config: {
            Temperature(0.2)        // Analytical and precise
            MaxOutputTokens(300)
        },
        input: {
            user([
                .text("Analyze this business data and provide insights:"),
                .text("```json\n\(jsonData)\n```"),
                .text("Please provide:"),
                .text("1. Revenue trends and predictions"),
                .text("2. Customer growth analysis"),
                .text("3. Profitability assessment"),
                .text("4. Recommendations for improvement")
            ])
        }
    )

    print("ðŸ“ˆ Analyzing business data...")
    let response = try await client.respond(to: request)

    if let insights = response.choices.first?.message.content {
        print("ðŸ¤– Business Analysis:")
        print("   \(insights)")
    }

    print("âœ… Structured data analysis completed!")
}

/// Example 7: Multimodal Conversation
/// Shows how to combine different content types in a conversation
func multimodalConversationExample() async throws {
    print("\nðŸŽ­ Multimodal Conversation")
    print("=========================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    var conversation = ResponseConversation()

    conversation.append(system: "You are a creative art teacher helping students understand visual composition.")

    // First message with text and image
    conversation.append(user: [
        .text("I'm trying to improve my photography composition. Here's a photo I took:"),
        .imageUrl(url: "https://picsum.photos/600/400?random=5", detail: .high),
        .text("What principles of composition can you identify? Any suggestions for improvement?")
    ])

    let firstResponse = try await client.chat(conversation: conversation)
    conversation.append(response: firstResponse)

    if let analysis = firstResponse.choices.first?.message.content {
        print("ðŸ¤– Composition Analysis:")
        print("   \(analysis)")
    }

    // Follow-up with additional context
    conversation.append(user: [
        .text("That's really helpful! For context, this was taken during golden hour."),
        .text("Here's another shot from the same location with different lighting:"),
        .imageUrl(url: "https://picsum.photos/600/400?random=6", detail: .high),
        .text("How does the lighting affect the composition?")
    ])

    let secondResponse = try await client.chat(conversation: conversation)
    conversation.append(response: secondResponse)

    if let comparison = secondResponse.choices.first?.message.content {
        print("\nðŸ¤– Lighting Comparison:")
        print("   \(comparison)")
    }

    print("âœ… Multimodal conversation completed!")
}

/// Main function to run all multimodal examples
func runMultimodalExamples() async {
    print("ðŸ–¼ï¸  SwiftResponsesDSL - Multimodal Content")
    print("========================================")
    print("Learn how to work with images, files, and structured data.\n")

    do {
        try await imageAnalysisExample()
        try await imageDetailComparisonExample()
        try await multipleImagesExample()
        try await imageWithContextExample()
        try await fileContentExample()
        try await structuredDataExample()
        try await multimodalConversationExample()

        print("\nðŸŽ‰ All multimodal examples completed!")
        print("ðŸ’¡ Next: Explore Advanced examples for tools and complex integrations!")

    } catch {
        print("âŒ Example failed with error:", error.localizedDescription)
        print("ðŸ’¡ Note: Some examples require vision-capable models (like gpt-4-vision-preview)")
    }
}
