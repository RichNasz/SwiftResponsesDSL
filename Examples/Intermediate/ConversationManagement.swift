//
//  ConversationManagement.swift
//  SwiftResponsesDSL Examples
//
//  Intermediate examples showing how to manage conversation history
//  and maintain context across multiple interactions.
//
//  Generated by AI-assisted code generation.
//  Created by Richard Naszcyniec on [Date].
//  Copyright ¬© [Year] Richard Naszcyniec. All rights reserved.
//

import SwiftResponsesDSL

/// Example 1: Basic Conversation Flow
/// Shows how to maintain context across multiple exchanges
func basicConversationExample() async throws {
    print("üí¨ Basic Conversation Management")
    print("===============================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Create a conversation object to track history
    var conversation = ResponseConversation()

    // Add initial context
    conversation.append(system: "You are a helpful programming tutor who explains concepts clearly.")

    let exchanges = [
        "What is recursion?",
        "Can you give me a simple example in Swift?",
        "How does it compare to iteration?"
    ]

    print("ü§ñ Starting conversation...")

    for (index, userMessage) in exchanges.enumerated() {
        print("\nüìù Exchange \(index + 1):")
        print("üë§ User: \(userMessage)")

        // Add user message to conversation
        conversation.append(user: userMessage)

        // Get response using the conversation
        let response = try await client.chat(conversation: conversation)
        conversation.append(response: response)

        if let assistantMessage = response.choices.first?.message.content {
            print("ü§ñ Assistant: \(assistantMessage)")
        }

        // Add a small delay to make the conversation flow naturally
        try await Task.sleep(nanoseconds: 500_000_000) // 0.5 seconds
    }

    print("\nüìä Conversation Summary:")
    print("   ‚Ä¢ Total exchanges: \(conversation.messages.count)")
    print("   ‚Ä¢ System messages: \(conversation.messages.filter { $0.role == .system }.count)")
    print("   ‚Ä¢ User messages: \(conversation.messages.filter { $0.role == .user }.count)")
    print("   ‚Ä¢ Assistant messages: \(conversation.messages.filter { $0.role == .assistant }.count)")

    print("‚úÖ Basic conversation completed!")
}

/// Example 2: Conversation with Custom Context Management
/// Shows how to build more sophisticated conversation flows
func advancedConversationExample() async throws {
    print("\nüé≠ Advanced Conversation with Context")
    print("====================================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    // Start with a specific role and context
    var conversation = ResponseConversation()

    conversation.append(system: """
    You are an expert software architect helping design a mobile app.
    Always consider:
    - User experience and accessibility
    - Performance and scalability
    - Security and privacy
    - Platform-specific best practices
    Provide specific, actionable advice.
    """)

    // Simulate a design discussion
    let designQuestions = [
        "I'm building a fitness tracking app. What are the key features I should include?",
        "How should I handle user data privacy in this app?",
        "What architecture pattern would you recommend for this type of app?"
    ]

    for question in designQuestions {
        print("\nüèóÔ∏è  Design Question:")
        print("üë§ \(question)")

        conversation.append(user: question)

        let response = try await client.chat(
            conversation: conversation,
            config: {
                Temperature(0.7)        // Creative but focused
                MaxOutputTokens(300)    // Allow detailed responses
            }
        )

        conversation.append(response: response)

        if let advice = response.choices.first?.message.content {
            print("ü§ñ Expert Advice: \(advice)")
        }
    }

    print("‚úÖ Advanced conversation completed!")
}

/// Example 3: Conversation Branching and Comparison
/// Shows how different conversation paths can lead to different outcomes
func conversationBranchingExample() async throws {
    print("\nüå≥ Conversation Branching Example")
    print("=================================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    let basePrompt = "Write a creative story about a time traveler"

    // Create two different conversation branches
    let branches = [
        ("Humorous", "Make it funny and light-hearted"),
        ("Dramatic", "Make it serious and suspenseful")
    ]

    for (style, instruction) in branches {
        print("\nüìñ Branch: \(style) Style")
        print("   Instruction: \(instruction)")

        var conversation = ResponseConversation()
        conversation.append(system: "You are a creative storyteller. \(instruction).")
        conversation.append(user: basePrompt)

        let response = try await client.chat(conversation: conversation)
        conversation.append(response: response)

        if let story = response.choices.first?.message.content {
            // Show first 200 characters of each story
            let preview = String(story.prefix(200))
            print("   üìù Story Preview: \(preview)...")
        }
    }

    print("‚úÖ Branching comparison completed!")
}

/// Example 4: Conversation with Memory Management
/// Shows how to manage conversation length and prevent token overflow
func conversationMemoryManagementExample() async throws {
    print("\nüß† Conversation Memory Management")
    print("=================================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    var conversation = ResponseConversation()

    // Start with context
    conversation.append(system: "You are a helpful assistant with excellent memory.")

    // Simulate a long conversation
    for i in 1...10 {
        let userMessage = "This is message number \(i). Remember that I mentioned the number \(i)."
        conversation.append(user: userMessage)

        // Keep conversation manageable by limiting history
        if conversation.messages.count > 15 {
            // Keep system message and last 7 exchanges (14 messages)
            let systemMessage = conversation.messages.first!
            let recentMessages = conversation.messages.suffix(14)
            conversation = ResponseConversation()
            conversation.append(message: systemMessage)
            recentMessages.forEach { conversation.append(message: $0) }
        }

        let response = try await client.chat(conversation: conversation)
        conversation.append(response: response)

        if let reply = response.choices.first?.message.content {
            print("üìù Exchange \(i): Assistant remembers context - \(reply.count > 50 ? "‚úì" : "‚úó")")
        }
    }

    print("\nüìä Memory Management Results:")
    print("   ‚Ä¢ Final conversation length: \(conversation.messages.count) messages")
    print("   ‚Ä¢ Memory efficiently managed: ‚úì")

    print("‚úÖ Memory management example completed!")
}

/// Example 5: Multi-turn Problem Solving
/// Shows how conversations can build upon previous responses for complex tasks
func multiTurnProblemSolvingExample() async throws {
    print("\nüß© Multi-turn Problem Solving")
    print("============================")

    let client = try LLMClient(baseURLString: "https://api.openai.com/v1/responses")

    var conversation = ResponseConversation()

    conversation.append(system: """
    You are an expert problem solver. Help the user work through complex problems
    step by step. Ask clarifying questions when needed and build upon previous
    responses to develop comprehensive solutions.
    """)

    // Complex problem that requires multiple steps
    let problem = """
    I'm trying to optimize my Swift app's performance. The app loads a large list
    of items from a remote API and displays them in a table view. Users report
    that scrolling is slow and the app feels laggy. What should I investigate?
    """

    conversation.append(user: problem)

    // First response
    let initialResponse = try await client.chat(conversation: conversation)
    conversation.append(response: initialResponse)

    if let initialAdvice = initialResponse.choices.first?.message.content {
        print("ü§ñ Initial Analysis:")
        print("   \(initialAdvice)")
    }

    // Follow-up questions to build on the solution
    let followUps = [
        "The API returns JSON with 1000+ items. Each item has 10+ fields. How should I handle parsing?",
        "I'm using UIKit table views. What specific optimizations can I implement?",
        "How should I handle images in the list items for best performance?"
    ]

    for followUp in followUps {
        print("\nüìù Follow-up: \(followUp)")

        conversation.append(user: followUp)

        let followUpResponse = try await client.chat(
            conversation: conversation,
            config: {
                Temperature(0.3)        // More focused for technical advice
                MaxOutputTokens(250)    // Allow detailed technical responses
            }
        )

        conversation.append(response: followUpResponse)

        if let advice = followUpResponse.choices.first?.message.content {
            print("ü§ñ Technical Solution:")
            print("   \(advice)")
        }
    }

    print("\nüìä Problem Solving Summary:")
    print("   ‚Ä¢ Problem analyzed: ‚úì")
    print("   ‚Ä¢ Multiple solutions explored: ‚úì")
    print("   ‚Ä¢ Technical depth achieved: ‚úì")

    print("‚úÖ Multi-turn problem solving completed!")
}

/// Main function to run all conversation management examples
func runConversationExamples() async {
    print("üí¨ SwiftResponsesDSL - Conversation Management")
    print("=============================================")
    print("Learn how to maintain context and build sophisticated interactions.\n")

    do {
        try await basicConversationExample()
        try await advancedConversationExample()
        try await conversationBranchingExample()
        try await conversationMemoryManagementExample()
        try await multiTurnProblemSolvingExample()

        print("\nüéâ All conversation examples completed!")
        print("üí° Next: Explore Advanced examples for complex integrations!")

    } catch {
        print("‚ùå Example failed with error:", error.localizedDescription)
    }
}
