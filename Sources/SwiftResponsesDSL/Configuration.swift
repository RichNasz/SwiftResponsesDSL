//
//  Configuration.swift
//  SwiftResponsesDSL
//
//  Configuration parameter structs for controlling LLM behavior
//
//  This file contains type-safe configuration parameters that control various
//  aspects of Large Language Model responses, including creativity, length,
//  repetition, and other behavioral characteristics.
//
//  Generated by AI-assisted code generation.
//  Created by Richard Naszcyniec on [Date].
//  Copyright Â© [Year] Richard Naszcyniec. All rights reserved.
//

import Foundation

// MARK: - Configuration Parameter Structs

/// Controls the randomness and creativity of LLM responses.
///
/// Temperature influences how deterministic the model's responses are.
/// Lower values (closer to 0.0) make responses more focused and deterministic,
/// while higher values (closer to 2.0) make responses more creative and varied.
///
/// - Note: Temperature interacts with other parameters like `TopP`.
///         A temperature of 0.0 with TopP can still produce varied results.
///
/// - Important: Values outside the 0.0...2.0 range will throw an error.
///              Most use cases work well with values between 0.0 and 1.0.
///
/// - SeeAlso: ``TopP``, ``ResponseRequest``
public struct Temperature: ResponseConfigParameter {
    /// The temperature value to apply
    public let value: Double

    /// Creates a temperature parameter with validation.
    ///
    /// - Parameter value: The temperature value (0.0 = focused, 2.0 = creative)
    /// - Throws: `LLMError.invalidValue` if the value is outside the valid range
    ///
    /// - Example:
    ///   ```swift
    ///   let temp = try Temperature(0.7)  // Balanced creativity
    ///   ```
    public init(_ value: Double) throws {
        guard (0.0...2.0).contains(value) else {
            throw LLMError.invalidValue("Temperature must be between 0.0 and 2.0")
        }
        self.value = value
    }

    /// Applies this temperature setting to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.temperature = value
    }
}

/// Controls response diversity using nucleus sampling.
///
/// TopP (nucleus sampling) considers only the most likely tokens whose
/// cumulative probability exceeds the threshold. This provides more
/// consistent results than temperature alone.
///
/// - Note: TopP and temperature can be used together for fine control.
///         Typical values are between 0.1 and 0.9.
///
/// - Important: Values must be between 0.0 and 1.0. A value of 1.0
///              considers all possible tokens.
///
/// - SeeAlso: ``Temperature``, ``ResponseRequest``
public struct TopP: ResponseConfigParameter {
    /// The nucleus sampling threshold
    public let value: Double

    /// Creates a TopP parameter with validation.
    ///
    /// - Parameter value: The cumulative probability threshold (0.0 = deterministic, 1.0 = all tokens)
    /// - Throws: `LLMError.invalidValue` if the value is outside the valid range
    ///
    /// - Example:
    ///   ```swift
    ///   let topp = try TopP(0.9)  // Consider top 90% of probability mass
    ///   ```
    public init(_ value: Double) throws {
        guard (0.0...1.0).contains(value) else {
            throw LLMError.invalidValue("TopP must be between 0.0 and 1.0")
        }
        self.value = value
    }

    /// Applies this TopP setting to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.topP = value
    }
}

/// Limits the maximum number of tokens in the response.
///
/// This parameter controls the length of generated responses by setting
/// an upper bound on the number of tokens that can be produced.
///
/// - Note: Token limits include both input and output tokens in the
///         conversation context. Very short limits may produce incomplete responses.
///
/// - Important: Must be a positive integer. Consider the model's context
///              window when setting this value.
///
/// - SeeAlso: ``ResponseRequest``
public struct MaxOutputTokens: ResponseConfigParameter {
    public let value: Int
    public init(_ value: Int) throws {
        guard value > 0 else {
            throw LLMError.invalidValue("MaxOutputTokens must be positive")
        }
        self.value = value
    }
    public func apply(to request: inout ResponseRequest) throws {
        request.maxOutputTokens = value
    }
}

/// Reduces the likelihood of the model repeating tokens it has already used.
///
/// Frequency penalty discourages repetition of specific words or phrases by
/// penalizing tokens based on how frequently they appear in the generated text.
///
/// - Note: Positive values reduce repetition, negative values encourage it.
///         A value of 0.0 applies no penalty.
///
/// - Important: Values must be between -2.0 and 2.0. Higher positive values
///              create more diverse responses.
///
/// - SeeAlso: ``PresencePenalty``, ``ResponseRequest``
public struct FrequencyPenalty: ResponseConfigParameter {
    /// The frequency penalty value
    public let value: Double

    /// Creates a frequency penalty parameter with validation.
    ///
    /// - Parameter value: The penalty value (-2.0 = encourage repetition, 2.0 = discourage repetition)
    /// - Throws: `LLMError.invalidValue` if the value is outside the valid range
    ///
    /// - Example:
    ///   ```swift
    ///   let penalty = try FrequencyPenalty(0.3)  // Moderate repetition reduction
    ///   ```
    public init(_ value: Double) throws {
        guard (-2.0...2.0).contains(value) else {
            throw LLMError.invalidValue("FrequencyPenalty must be between -2.0 and 2.0")
        }
        self.value = value
    }

    /// Applies this frequency penalty to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.frequencyPenalty = value
    }
}

/// Reduces the likelihood of the model repeating topics it has already discussed.
///
/// Presence penalty discourages repetition of topics and concepts by penalizing
/// tokens that represent ideas already present in the conversation.
///
/// - Note: This works differently from frequency penalty - it penalizes concepts
///         rather than specific words. Positive values encourage topic diversity.
///
/// - Important: Values must be between -2.0 and 2.0. Use in conjunction with
///              frequency penalty for comprehensive repetition control.
///
/// - SeeAlso: ``FrequencyPenalty``, ``ResponseRequest``
public struct PresencePenalty: ResponseConfigParameter {
    /// The presence penalty value
    public let value: Double

    /// Creates a presence penalty parameter with validation.
    ///
    /// - Parameter value: The penalty value (-2.0 = encourage topic repetition, 2.0 = encourage diversity)
    /// - Throws: `LLMError.invalidValue` if the value is outside the valid range
    ///
    /// - Example:
    ///   ```swift
    ///   let penalty = try PresencePenalty(0.2)  // Encourage topic diversity
    ///   ```
    public init(_ value: Double) throws {
        guard (-2.0...2.0).contains(value) else {
            throw LLMError.invalidValue("PresencePenalty must be between -2.0 and 2.0")
        }
        self.value = value
    }

    /// Applies this presence penalty to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.presencePenalty = value
    }
}

/// Limits the maximum number of tool calls the model can make in a single response.
///
/// This parameter controls how many tools the LLM can invoke when responding
/// to a request that involves tool usage.
///
/// - Note: Tool calls allow the LLM to interact with external systems and APIs.
///         This limit prevents excessive tool usage in a single response.
///
/// - Important: Must be between 1 and 128. Consider the complexity of your
///              task when setting this limit.
///
/// - SeeAlso: ``Tool``, ``ResponseRequest``
public struct MaxToolCalls: ResponseConfigParameter {
    /// The maximum number of tool calls allowed
    public let value: Int

    /// Creates a max tool calls parameter with validation.
    ///
    /// - Parameter value: The maximum number of tool calls (1-128)
    /// - Throws: `LLMError.invalidValue` if the value is outside the valid range
    ///
    /// - Example:
    ///   ```swift
    ///   let maxCalls = try MaxToolCalls(5)  // Allow up to 5 tool calls
    ///   ```
    public init(_ value: Int) throws {
        guard (1...128).contains(value) else {
            throw LLMError.invalidValue("MaxToolCalls must be between 1 and 128")
        }
        self.value = value
    }

    /// Applies this tool call limit to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.maxToolCalls = value
    }
}

/// Controls when and how the model uses tools in responses.
///
/// Tool choice determines the model's behavior regarding tool usage,
/// allowing you to control whether tools are used automatically, required,
/// or not used at all.
///
/// - Note: "auto" allows the model to decide when to use tools, "required"
///         forces tool usage, and "none" prevents tool usage entirely.
///
/// - Important: Must be one of: "none", "auto", "required". This setting
///              significantly affects how the model interacts with tools.
///
/// - SeeAlso: ``Tool``, ``MaxToolCalls``, ``ResponseRequest``
public struct ToolChoice: ResponseConfigParameter {
    /// The tool choice behavior setting
    public let value: String

    /// Creates a tool choice parameter with validation.
    ///
    /// - Parameter value: The tool choice behavior ("none", "auto", "required")
    /// - Throws: `LLMError.invalidValue` if the value is not a valid choice
    ///
    /// - Example:
    ///   ```swift
    ///   let choice = try ToolChoice("auto")  // Let model decide when to use tools
    ///   ```
    public init(_ value: String) throws {
        let validChoices = ["none", "auto", "required"]
        guard validChoices.contains(value) else {
            throw LLMError.invalidValue("ToolChoice must be one of: \(validChoices.joined(separator: ", "))")
        }
        self.value = value
    }

    /// Applies this tool choice setting to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.toolChoice = value
    }
}

/// Returns the top token log probabilities for analysis and debugging.
///
/// This parameter controls how many of the most likely tokens and their
/// log probabilities are returned alongside the response text.
///
/// - Note: Log probabilities help understand the model's confidence in its
///         token choices. Higher values provide more detailed probability information.
///
/// - Important: Must be between 0 and 20. Higher values increase response size
///              and processing time. A value of 0 disables log probability output.
///
/// - SeeAlso: ``ResponseRequest``
public struct TopLogprobs: ResponseConfigParameter {
    /// The number of top log probabilities to return
    public let value: Int

    /// Creates a top logprobs parameter with validation.
    ///
    /// - Parameter value: The number of top probabilities to return (0-20)
    /// - Throws: `LLMError.invalidValue` if the value is outside the valid range
    ///
    /// - Example:
    ///   ```swift
    ///   let logprobs = try TopLogprobs(5)  // Return top 5 probabilities
    ///   ```
    public init(_ value: Int) throws {
        guard (0...20).contains(value) else {
            throw LLMError.invalidValue("TopLogprobs must be between 0 and 20")
        }
        self.value = value
    }

    /// Applies this log probability setting to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.topLogprobs = value
    }
}

/// Sets a random seed for reproducible model outputs.
///
/// Using a seed ensures that the model generates identical outputs for
/// identical inputs, which is useful for testing and reproducible results.
///
/// - Note: While seeds improve reproducibility, they don't guarantee identical
///         results across different model versions or API changes.
///
/// - Important: Must be a non-negative integer. Use the same seed value
///              to reproduce previous results.
///
/// - SeeAlso: ``Temperature``, ``ResponseRequest``
public struct Seed: ResponseConfigParameter {
    /// The random seed value
    public let value: Int

    /// Creates a seed parameter with validation.
    ///
    /// - Parameter value: The seed value (must be non-negative)
    /// - Throws: `LLMError.invalidValue` if the value is negative
    ///
    /// - Example:
    ///   ```swift
    ///   let seed = try Seed(42)  // Use seed 42 for reproducible results
    ///   ```
    public init(_ value: Int) throws {
        guard value >= 0 else {
            throw LLMError.invalidValue("Seed must be non-negative")
        }
        self.value = value
    }

    /// Applies this seed to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.seed = value
    }
}

/// Defines the tools available to the model for function calling.
///
/// This parameter specifies which tools the LLM can invoke when responding
/// to requests that require external functionality or data access.
///
/// - Note: Tools enable the LLM to perform actions like API calls, database
///         queries, or computations beyond its built-in capabilities.
///
/// - Important: The tools array cannot be empty when provided. Each tool
///              must have a valid name, description, and parameter schema.
///
/// - SeeAlso: ``Tool``, ``ToolChoice``, ``MaxToolCalls``, ``ResponseRequest``
public struct Tools: ResponseConfigParameter {
    /// The array of available tools
    public let value: [Tool]

    /// Creates a tools parameter with validation.
    ///
    /// - Parameter value: Array of tools to make available to the model
    /// - Throws: `LLMError.invalidValue` if the tools array is empty
    ///
    /// - Example:
    ///   ```swift
    ///   let tools = try Tools([
    ///       Tool(type: .function, function: Tool.Function(
    ///           name: "get_weather",
    ///           description: "Get weather information",
    ///           parameters: .object([
    ///               "location": .string(description: "City name")
    ///           ])
    ///       ))
    ///   ])
    ///   ```
    public init(_ value: [Tool]) throws {
        guard !value.isEmpty else {
            throw LLMError.invalidValue("Tools array cannot be empty")
        }
        self.value = value
    }

    /// Applies these tools to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.tools = value
    }
}

/// Controls whether the model can make multiple tool calls in parallel.
///
/// When enabled, the model can invoke multiple tools simultaneously rather
/// than waiting for each tool call to complete before making the next.
///
/// - Note: Parallel tool calls can significantly improve performance for
///         requests that require multiple independent tool invocations.
///
/// - Important: Not all models support parallel tool calls. Check your
///              model's capabilities before enabling this feature.
///
/// - SeeAlso: ``Tool``, ``Tools``, ``ResponseRequest``
public struct ParallelToolCalls: ResponseConfigParameter {
    /// Whether to enable parallel tool calls
    public let value: Bool

    /// Creates a parallel tool calls parameter.
    ///
    /// - Parameter value: Whether to enable parallel tool calls
    ///
    /// - Example:
    ///   ```swift
    ///   let parallel = ParallelToolCalls(true)  // Enable parallel calls
    ///   ```
    public init(_ value: Bool) {
        self.value = value
    }

    /// Applies this parallel tool calls setting to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.parallelToolCalls = value
    }
}

/// Configures options for streaming responses.
///
/// This parameter allows fine-tuning of streaming behavior, such as including
/// usage statistics or controlling the streaming format.
///
/// - Note: Streaming options are passed through to the LLM API and may vary
///         depending on the specific model and provider being used.
///
/// - Important: The available options depend on your LLM provider's API.
///              Common options include usage statistics and formatting preferences.
///
/// - SeeAlso: ``ResponseRequest``
public struct StreamOptions: ResponseConfigParameter {
    /// The streaming configuration options
    public let value: [String: AnyCodable]

    /// Creates a stream options parameter.
    ///
    /// - Parameter value: Dictionary of streaming options
    ///
    /// - Example:
    ///   ```swift
    ///   let options = StreamOptions([
    ///       "include_usage": .bool(true),
    ///       "temperature": .double(0.7)
    ///   ])
    ///   ```
    public init(_ value: [String: AnyCodable]) {
        self.value = value
    }

    /// Applies these streaming options to a request.
    ///
    /// - Parameter request: The request to modify
    /// - Throws: Never, but required by protocol
    public func apply(to request: inout ResponseRequest) throws {
        request.streamOptions = value
    }
}
