//
//  Messages.swift
//  SwiftResponsesDSL
//
//  Message types and content parts for the SwiftResponsesDSL
//
//  This file defines the multimodal content structures and message types used
//  throughout SwiftResponsesDSL for communication with Large Language Models.
//
//  Generated by AI-assisted code generation.
//  Created by Richard Naszcyniec on [Date].
//  Copyright Â© [Year] Richard Naszcyniec. All rights reserved.
//

import Foundation

// MARK: - Content Parts

/// A representation of multimodal content that can be included in messages.
///
/// `ContentPart` supports various types of content including text, images, and files,
/// enabling rich multimodal interactions with LLMs. Each content part is encoded
/// according to the OpenAI-compatible API specifications.
///
/// - Note: The encoding format follows OpenAI's multimodal API structure,
///         ensuring compatibility across different LLM providers.
///
/// - SeeAlso: ``ResponseMessage``, ``SystemMessage``, ``UserMessage``
public enum ContentPart: Codable, Sendable {
    /// Plain text content
    ///
    /// - Parameter text: The text content to include
    /// - Note: This is the most common content type for conversational AI
    case text(String)

    /// An image referenced by URL
    ///
    /// - Parameters:
    ///   - url: The URL of the image to include
    ///   - detail: Optional detail level for image processing (auto, low, high)
    /// - Note: The image will be downloaded and processed by the LLM.
    ///         Detail level affects processing quality and token usage.
    case imageUrl(url: String, detail: Detail? = nil)

    /// A file referenced by its file ID
    ///
    /// - Parameter fileId: The unique identifier of an uploaded file
    /// - Note: Files must be pre-uploaded to the LLM provider's storage
    case inputFile(fileId: String)

    /// File data encoded as a data URL
    ///
    /// - Parameter dataUrl: Base64-encoded file data with MIME type
    /// - Note: Useful for small files that can be included directly.
    ///         Format: `data:mime/type;base64,<base64-data>`
    case inputFileData(dataUrl: String)

    /// Detail level for image processing
    ///
    /// Controls the level of detail used when processing images.
    /// Higher detail levels provide better analysis but use more tokens.
    ///
    /// - SeeAlso: ``imageUrl(url:detail:)``
    public enum Detail: String, Codable, Sendable {
        /// Automatically choose the appropriate detail level
        case auto

        /// Low detail processing (fewer tokens, faster)
        case low

        /// High detail processing (more tokens, slower, more accurate)
        case high
    }

    public func encode(to encoder: Encoder) throws {
        var container = encoder.container(keyedBy: CodingKeys.self)
        switch self {
        case .text(let content):
            try container.encode("text", forKey: .type)
            try container.encode(content, forKey: .text)
        case .imageUrl(let url, let detail):
            try container.encode("image_url", forKey: .type)
            var imageUrlContainer = container.nestedContainer(keyedBy: ImageUrlKeys.self, forKey: .imageUrl)
            try imageUrlContainer.encode(url, forKey: .url)
            if let detail = detail {
                try imageUrlContainer.encode(detail, forKey: .detail)
            }
        case .inputFile(let fileId):
            try container.encode("input_file", forKey: .type)
            try container.encode(fileId, forKey: .fileId)
        case .inputFileData(let dataUrl):
            try container.encode("input_file_data", forKey: .type)
            try container.encode(dataUrl, forKey: .fileData)
        }
    }

    public init(from decoder: Decoder) throws {
        let container = try decoder.container(keyedBy: CodingKeys.self)
        let type = try container.decode(String.self, forKey: .type)

        switch type {
        case "text":
            let text = try container.decode(String.self, forKey: .text)
            self = .text(text)
        case "image_url":
            let imageUrlContainer = try container.nestedContainer(keyedBy: ImageUrlKeys.self, forKey: .imageUrl)
            let url = try imageUrlContainer.decode(String.self, forKey: .url)
            let detail = try imageUrlContainer.decodeIfPresent(Detail.self, forKey: .detail)
            self = .imageUrl(url: url, detail: detail)
        case "input_file":
            let fileId = try container.decode(String.self, forKey: .fileId)
            self = .inputFile(fileId: fileId)
        case "input_file_data":
            let dataUrl = try container.decode(String.self, forKey: .fileData)
            self = .inputFileData(dataUrl: dataUrl)
        default:
            throw DecodingError.dataCorruptedError(forKey: .type, in: container, debugDescription: "Unknown content part type: \(type)")
        }
    }

    private enum CodingKeys: String, CodingKey {
        case type, text, imageUrl = "image_url", fileId, fileData
    }

    private enum ImageUrlKeys: String, CodingKey {
        case url, detail
    }
}

/// Represents annotations in response text such as citations and references.
///
/// Annotations provide additional context and source information for content
/// generated by LLMs, including citations to web pages, files, or other sources.
///
/// - Note: Annotations help track the sources of information used in LLM responses,
///         improving transparency and verifiability of generated content.
///
/// - SeeAlso: ``Response``, ``ContentPart``
public enum Annotation: Decodable, Sendable {
    /// A citation to a web page or URL
    ///
    /// - Parameters:
    ///   - startIndex: Character index where the citation starts in the text
    ///   - endIndex: Character index where the citation ends in the text
    ///   - url: The URL being cited
    ///   - title: Title or description of the cited content
    case urlCitation(startIndex: Int, endIndex: Int, url: String, title: String)

    /// A citation to a file
    ///
    /// - Parameters:
    ///   - index: Citation index number
    ///   - fileId: Unique identifier of the cited file
    ///   - filename: Name of the cited file
    case fileCitation(index: Int, fileId: String, filename: String)

    /// An unknown annotation type
    ///
    /// - Parameters:
    ///   - type: The annotation type string
    ///   - data: Additional annotation data
    /// - Note: Used for forward compatibility with new annotation types
    case unknown(type: String, data: [String: AnyCodable])

    public init(from decoder: Decoder) throws {
        let container = try decoder.container(keyedBy: CodingKeys.self)
        let type = try container.decode(String.self, forKey: .type)

        switch type {
        case "url_citation":
            let startIndex = try container.decode(Int.self, forKey: .startIndex)
            let endIndex = try container.decode(Int.self, forKey: .endIndex)
            let url = try container.decode(String.self, forKey: .url)
            let title = try container.decode(String.self, forKey: .title)
            self = .urlCitation(startIndex: startIndex, endIndex: endIndex, url: url, title: title)
        case "file_citation":
            let index = try container.decode(Int.self, forKey: .index)
            let fileId = try container.decode(String.self, forKey: .fileId)
            let filename = try container.decode(String.self, forKey: .filename)
            self = .fileCitation(index: index, fileId: fileId, filename: filename)
        default:
            let data = try container.decode([String: AnyCodable].self, forKey: .data)
            self = .unknown(type: type, data: data)
        }
    }

    private enum CodingKeys: String, CodingKey {
        case type, startIndex = "start_index", endIndex = "end_index", url, title, index, fileId = "file_id", filename, data
    }
}

// MARK: - Message Types

/// A message containing system instructions and context for the LLM.
///
/// System messages provide high-level instructions, personality traits, and behavioral
/// guidelines for the LLM. They are typically set at the beginning of a conversation
/// and influence how the LLM responds throughout the interaction.
///
/// - Note: System messages are processed differently by LLMs and often have
///         stronger influence on behavior than other message types.
///
/// - Important: Place system messages at the beginning of conversations for
///              maximum effectiveness.
///
/// - SeeAlso: ``UserMessage``, ``AssistantMessage``, ``ResponseMessage``
public struct SystemMessage: ResponseMessage {
    /// The message role, always `.system`
    public let role: Role = .system

    /// The multimodal content of the system message
    public let content: [ContentPart]

    /// Creates a system message with text content.
    ///
    /// - Parameter text: The system instructions or context
    /// - Note: This is the most common way to create system messages
    ///
    /// - SeeAlso: ``init(content:)``
    public init(text: String) {
        self.content = [.text(text)]
    }

    /// Creates a system message with multimodal content.
    ///
    /// - Parameter content: Array of content parts including text, images, etc.
    /// - Note: Useful for system messages that include visual context or examples
    ///
    /// - SeeAlso: ``init(text:)``
    public init(content: [ContentPart]) {
        self.content = content
    }

    public func encode(to encoder: Encoder) throws {
        var container = encoder.container(keyedBy: CodingKeys.self)
        try container.encode(role, forKey: .role)
        try container.encode(content, forKey: .content)
    }

    private enum CodingKeys: String, CodingKey {
        case role, content
    }
}

/// A message representing input from a user or tool execution results.
///
/// User messages contain the primary input to the LLM, including text, images, files,
/// and other multimodal content. They can also represent the results of tool executions
/// when the role is set to `.tool`.
///
/// - Note: User messages are the primary way to provide input to LLMs in SwiftResponsesDSL.
///         They support rich multimodal content for comprehensive interactions.
///
/// - Important: When using `.tool` role, the message represents the output of a tool execution.
///
/// - SeeAlso: ``SystemMessage``, ``AssistantMessage``, ``ResponseMessage``
public struct UserMessage: ResponseMessage {
    /// The role of this message (`.user` or `.tool`)
    public let role: Role

    /// The multimodal content of the user message
    public let content: [ContentPart]

    /// Creates a user message with multimodal content.
    ///
    /// - Parameters:
    ///   - role: The message role (defaults to `.user`)
    ///   - content: Array of content parts to include
    /// - Note: Use this initializer for complex multimodal messages
    ///
    /// - SeeAlso: ``init(role:text:)``, ``ContentPart``
    public init(role: Role = .user, content: [ContentPart]) {
        self.role = role
        self.content = content
    }

    /// Creates a user message with text content.
    ///
    /// - Parameters:
    ///   - role: The message role (defaults to `.user`)
    ///   - text: The text content to send
    /// - Note: This is the most common way to create user messages
    ///
    /// - SeeAlso: ``init(role:content:)``
    public init(role: Role = .user, text: String) {
        self.role = role
        self.content = [.text(text)]
    }

    /// Creates a user message with a file reference.
    ///
    /// - Parameters:
    ///   - role: The message role (defaults to `.user`)
    ///   - fileId: The unique identifier of an uploaded file
    /// - Note: The file must be pre-uploaded to the LLM provider
    ///
    /// - SeeAlso: ``init(role:base64File:mimeType:)``
    public init(role: Role = .user, fileId: String) {
        self.role = role
        self.content = [.inputFile(fileId: fileId)]
    }

    /// Creates a user message with inline file data.
    ///
    /// - Parameters:
    ///   - role: The message role (defaults to `.user`)
    ///   - base64File: Base64-encoded file data
    ///   - mimeType: MIME type of the file (defaults to "application/pdf")
    /// - Note: Useful for small files that can be included directly in the request
    ///
    /// - SeeAlso: ``init(role:fileId:)``
    public init(role: Role = .user, base64File: String, mimeType: String = "application/pdf") {
        self.role = role
        self.content = [.inputFileData(dataUrl: "data:\(mimeType);base64,\(base64File)")]
    }

    public func encode(to encoder: Encoder) throws {
        var container = encoder.container(keyedBy: CodingKeys.self)
        try container.encode(role, forKey: .role)
        try container.encode(content, forKey: .content)
    }

    private enum CodingKeys: String, CodingKey {
        case role, content
    }
}

/// A message containing responses generated by the LLM.
///
/// Assistant messages represent the output generated by Large Language Models in response
/// to user input. They can be created programmatically or decoded from API responses.
/// Assistant messages are typically added to conversation history to maintain context.
///
/// - Note: Assistant messages are primarily used for conversation history and are
///         usually created by decoding API responses rather than constructed manually.
///
/// - Important: When manually creating assistant messages for conversation history,
///              ensure they accurately represent the actual LLM response.
///
/// - SeeAlso: ``SystemMessage``, ``UserMessage``, ``ResponseMessage``
public struct AssistantMessage: ResponseMessage, Decodable {
    /// The message role, always `.assistant`
    public let role: Role = .assistant

    /// The multimodal content of the assistant message
    public let content: [ContentPart]

    /// Creates an assistant message with text content.
    ///
    /// - Parameter text: The text content of the assistant response
    /// - Note: Use this initializer when manually creating assistant messages
    ///         for conversation history or testing purposes.
    ///
    /// - SeeAlso: ``init(content:)``
    public init(text: String) {
        self.content = [.text(text)]
    }

    /// Creates an assistant message with multimodal content.
    ///
    /// - Parameter content: Array of content parts from the assistant response
    /// - Note: Use this initializer for complex multimodal assistant responses
    ///
    /// - SeeAlso: ``init(text:)``
    public init(content: [ContentPart]) {
        self.content = content
    }

    public init(from decoder: Decoder) throws {
        let container = try decoder.container(keyedBy: CodingKeys.self)
        let decodedRole = try container.decode(Role.self, forKey: .role)
        guard decodedRole == .assistant else {
            throw DecodingError.dataCorruptedError(forKey: .role, in: container, debugDescription: "Expected assistant role")
        }
        self.content = try container.decode([ContentPart].self, forKey: .content)
    }

    public func encode(to encoder: Encoder) throws {
        var container = encoder.container(keyedBy: CodingKeys.self)
        try container.encode(role, forKey: .role)
        try container.encode(content, forKey: .content)
    }

    private enum CodingKeys: String, CodingKey {
        case role, content
    }
}
